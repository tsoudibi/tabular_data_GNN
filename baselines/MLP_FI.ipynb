{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import torch_geometric\n",
    "from tqdm import tqdm, trange\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from torcheval.metrics import BinaryAUROC\n",
    "import numpy as np\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# DEVICE = torch.device('cuda')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48842, 14]) torch.Size([48842])\n",
      "[9, 16, 7, 15, 6, 5, 2, 42]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.995129</td>\n",
       "      <td>0.351675</td>\n",
       "      <td>-1.197259</td>\n",
       "      <td>-0.144804</td>\n",
       "      <td>-0.217127</td>\n",
       "      <td>-0.034087</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.046942</td>\n",
       "      <td>-0.945524</td>\n",
       "      <td>-0.419335</td>\n",
       "      <td>-0.144804</td>\n",
       "      <td>-0.217127</td>\n",
       "      <td>0.772930</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.776316</td>\n",
       "      <td>1.394723</td>\n",
       "      <td>0.747550</td>\n",
       "      <td>-0.144804</td>\n",
       "      <td>-0.217127</td>\n",
       "      <td>-0.034087</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.390683</td>\n",
       "      <td>-0.277844</td>\n",
       "      <td>-0.030373</td>\n",
       "      <td>0.886874</td>\n",
       "      <td>-0.217127</td>\n",
       "      <td>-0.034087</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.505691</td>\n",
       "      <td>-0.815954</td>\n",
       "      <td>-0.030373</td>\n",
       "      <td>-0.144804</td>\n",
       "      <td>-0.217127</td>\n",
       "      <td>-0.841104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>-0.849254</td>\n",
       "      <td>0.640492</td>\n",
       "      <td>0.747550</td>\n",
       "      <td>-0.144804</td>\n",
       "      <td>-0.217127</td>\n",
       "      <td>-0.195490</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>0.098933</td>\n",
       "      <td>-0.334178</td>\n",
       "      <td>-0.419335</td>\n",
       "      <td>-0.144804</td>\n",
       "      <td>-0.217127</td>\n",
       "      <td>-0.034087</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>1.411808</td>\n",
       "      <td>-0.357510</td>\n",
       "      <td>-0.419335</td>\n",
       "      <td>-0.144804</td>\n",
       "      <td>-0.217127</td>\n",
       "      <td>-0.034087</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>-1.213941</td>\n",
       "      <td>0.111984</td>\n",
       "      <td>-0.419335</td>\n",
       "      <td>-0.144804</td>\n",
       "      <td>-0.217127</td>\n",
       "      <td>-1.648120</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>0.974183</td>\n",
       "      <td>0.930494</td>\n",
       "      <td>-0.419335</td>\n",
       "      <td>1.871315</td>\n",
       "      <td>-0.217127</td>\n",
       "      <td>-0.034087</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48842 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age    fnlwgt  educational-num  capital-gain  capital-loss  \\\n",
       "0     -0.995129  0.351675        -1.197259     -0.144804     -0.217127   \n",
       "1     -0.046942 -0.945524        -0.419335     -0.144804     -0.217127   \n",
       "2     -0.776316  1.394723         0.747550     -0.144804     -0.217127   \n",
       "3      0.390683 -0.277844        -0.030373      0.886874     -0.217127   \n",
       "4     -1.505691 -0.815954        -0.030373     -0.144804     -0.217127   \n",
       "...         ...       ...              ...           ...           ...   \n",
       "48837 -0.849254  0.640492         0.747550     -0.144804     -0.217127   \n",
       "48838  0.098933 -0.334178        -0.419335     -0.144804     -0.217127   \n",
       "48839  1.411808 -0.357510        -0.419335     -0.144804     -0.217127   \n",
       "48840 -1.213941  0.111984        -0.419335     -0.144804     -0.217127   \n",
       "48841  0.974183  0.930494        -0.419335      1.871315     -0.217127   \n",
       "\n",
       "       hours-per-week  workclass  education  marital-status  occupation  \\\n",
       "0           -0.034087        4.0        1.0             4.0         7.0   \n",
       "1            0.772930        4.0       11.0             2.0         5.0   \n",
       "2           -0.034087        2.0        7.0             2.0        11.0   \n",
       "3           -0.034087        4.0       15.0             2.0         7.0   \n",
       "4           -0.841104        0.0       15.0             4.0         0.0   \n",
       "...               ...        ...        ...             ...         ...   \n",
       "48837       -0.195490        4.0        7.0             2.0        13.0   \n",
       "48838       -0.034087        4.0       11.0             2.0         7.0   \n",
       "48839       -0.034087        4.0       11.0             6.0         1.0   \n",
       "48840       -1.648120        4.0       11.0             4.0         1.0   \n",
       "48841       -0.034087        5.0       11.0             2.0         4.0   \n",
       "\n",
       "       relationship  race  gender  native-country  income  \n",
       "0               3.0   2.0     1.0            39.0       0  \n",
       "1               0.0   4.0     1.0            39.0       0  \n",
       "2               0.0   4.0     1.0            39.0       1  \n",
       "3               0.0   2.0     1.0            39.0       1  \n",
       "4               3.0   4.0     0.0            39.0       0  \n",
       "...             ...   ...     ...             ...     ...  \n",
       "48837           5.0   4.0     0.0            39.0       0  \n",
       "48838           0.0   4.0     1.0            39.0       1  \n",
       "48839           4.0   4.0     0.0            39.0       0  \n",
       "48840           3.0   4.0     1.0            39.0       0  \n",
       "48841           5.0   4.0     0.0            39.0       1  \n",
       "\n",
       "[48842 rows x 15 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get feature from csv\n",
    "RAW_data = pd.read_csv('../data/adult.csv')\n",
    "CAT = ['workclass','education','marital-status','occupation','relationship','race','gender','native-country']\n",
    "NUM = ['age','fnlwgt','educational-num','capital-gain','capital-loss','hours-per-week']\n",
    "LABEL = 'income'\n",
    "# RAW_data = pd.read_csv('../data/compass_old.csv')\n",
    "# CAT=['sex','age_cat','race','c_charge_degree','decile_score.1','score_text','v_type_of_assessment','v_decile_score','v_score_text']\n",
    "# NUM=['age','juv_fel_count','juv_misd_count','juv_other_count','priors_count','days_b_screening_arrest','c_days_from_compas','end']\n",
    "# LABEL = 'is_recid'\n",
    "# convert categorical data to ordinal data\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "enc = OrdinalEncoder()\n",
    "data_pd = RAW_data.copy()\n",
    "data_pd[CAT] = enc.fit_transform(RAW_data[CAT])\n",
    "# data_pd = pd.get_dummies(RAW_data, columns=CAT, dtype=float)\n",
    "# label to category\n",
    "data_pd[LABEL] = data_pd[LABEL].astype('category').cat.codes\n",
    "\n",
    "# realign data to num + cat\n",
    "data_pd = data_pd[NUM + CAT + [LABEL]]\n",
    "\n",
    "# caculate unique value of each categorical feature\n",
    "cat_num = [len(data_pd[col].unique()) for col in CAT]\n",
    "\n",
    "# normalize numerical data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_pd[NUM] = scaler.fit_transform(data_pd[NUM])\n",
    "\n",
    "# convert data to tensor\n",
    "x = torch.tensor(data_pd.drop(columns=[LABEL]).values, dtype=torch.float, device=DEVICE)  # [48842, 108]\n",
    "y = torch.tensor(data_pd[LABEL].values, dtype=torch.long, device=DEVICE) # [48842]\n",
    "print(x.shape, y.shape)\n",
    "print(cat_num)\n",
    "data_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_graph(torch.nn.Module):\n",
    "    def __init__(self, NUM, CAT, LABEL, cat_num):\n",
    "        super(K_graph, self).__init__()\n",
    "        '''\n",
    "        num_cols: number of numerical columns\n",
    "        cat_cols: number of categorical columns\n",
    "        label_cols: number of label columns\n",
    "        cat_num: number of unique value of each categorical columns\n",
    "        '''\n",
    "        self.hidden_dim = 128\n",
    "        \n",
    "        # order: num -> cat -> label\n",
    "        self.num_cols = len(NUM)\n",
    "        self.cat_cols = len(CAT)\n",
    "        self.label_cols = len(LABEL)\n",
    "        self.number_of_columns = self.num_cols + self.cat_cols \n",
    "        \n",
    "        \n",
    "        # numerical feature\n",
    "        self.num_embeddings = torch.nn.ModuleList([torch.nn.Linear(1, self.hidden_dim) for i in range(self.num_cols)])\n",
    "        # categorical feature\n",
    "        self.cat_embeddings = torch.nn.ModuleList([torch.nn.Embedding(cat_num[i], self.hidden_dim) for i in range(self.cat_cols)])\n",
    "        \n",
    "        self.prediction = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.hidden_dim * self.number_of_columns, self.hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.LayerNorm(self.hidden_dim),\n",
    "            torch.nn.Linear(self.hidden_dim, self.label_cols + 1)\n",
    "        )\n",
    "        \n",
    "        # feature importance learning\n",
    "        self.feature_importance_learners = torch.nn.ModuleList([torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.hidden_dim, 1),\n",
    "        ) for i in range(self.number_of_columns)])\n",
    "        \n",
    "        \n",
    "    def forward(self, input_data, epoch = -1):\n",
    "        \n",
    "        # make feature embedding\n",
    "        num_data = input_data[:,:self.num_cols].unsqueeze(-1).unsqueeze(-1) \n",
    "        feature_embedding_num = torch.cat([self.num_embeddings[i](num_data[:,i]) for i in range(self.num_cols)], dim=1).reshape(len(input_data), -1) # [batch_size, num_cols * hidden_dim]\n",
    "        feature_embedding_num = torch.nn.ReLU()(feature_embedding_num)\n",
    "        feature_embedding_num = torch.layer_norm(feature_embedding_num, feature_embedding_num.shape)\n",
    "        # categorical feature\n",
    "        feature_embedding_cat = torch.cat([self.cat_embeddings[i](input_data[:,self.num_cols+i].long()) for i in range(self.cat_cols)], dim=1) # [batch_size, cat_cols * hidden_dim]\n",
    "        feature_embedding_cat = torch.layer_norm(feature_embedding_cat, feature_embedding_cat.shape)\n",
    "        # concat\n",
    "        feature_embedding = torch.cat((feature_embedding_num, feature_embedding_cat), dim=1) # [batch_size, (num_cols + cat_cols) * hidden_dim]\n",
    "        # print(feature_embedding.shape)\n",
    "        # feature_embedding = feature_embedding.reshape((len(input_data), self.number_of_columns, -1)) # [batch_size, (num_cols + cat_cols), hidden_dim]\n",
    "        \n",
    "        # # feature importance learning\n",
    "        # feature_importance_ = torch.cat([self.feature_importance_learners[i](feature_embedding[:,i*self.hidden_dim:(i+1)*self.hidden_dim]) for i in range(self.number_of_columns)], dim=1) # [batch_size, num_cols + cat_cols, 1]\n",
    "        # feature_importance = torch.softmax(feature_importance_, dim=1) # [batch_size, num_cols + cat_cols, 1]\n",
    "        # feature_importance = torch.layer_norm(feature_importance, feature_importance.shape)\n",
    "        # # print('feature_importance',feature_importance.sum(dim=0)/len(input_data))\n",
    "        # feature_embedding = feature_embedding.reshape((len(input_data),self.number_of_columns, -1)) * feature_importance.unsqueeze(-1) # [batch_size, (num_cols + cat_cols) * hidden_dim]\n",
    "        # feature_embedding = feature_embedding.reshape((len(input_data), -1)) # [batch_size, (num_cols + cat_cols) * hidden_dim]\n",
    "        \n",
    "        \n",
    "        # make prediction\n",
    "        prediction = self.prediction(feature_embedding)\n",
    "        \n",
    "        \n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([48511, 30917, 26725, 21784,  4636, 28281, 23661, 14621, 48500, 28845,\n",
      "         9172, 11413,  8184, 24427, 39428, 24792, 45015,  1472, 47274, 42164,\n",
      "          618, 44699, 48374, 24245, 46329, 38705,  5791, 25825, 20397, 38938,\n",
      "        32714,  6203, 17434, 31866, 15415,  8377,  2962, 13995, 19808, 37027,\n",
      "        17970,  6518, 15183,  6568, 44975,  5160, 44515, 20400, 28787, 13316,\n",
      "        44306, 28330, 30658, 24987, 46105, 22075, 36253, 20690, 31050, 13061,\n",
      "        27015,    74, 30715, 20362, 16385,  4839,  8524, 35403, 36674, 44473,\n",
      "        39099, 10081, 19897, 17738, 33455, 35643,  1034, 24978, 34058, 25843,\n",
      "        44109,  6409,   945,  6433, 45752, 22484, 34206,  6658, 35371, 46258,\n",
      "        39902,  8322, 47861, 10925, 40932, 47223, 12875, 35988,  2969,  2387])\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "the_model = K_graph(NUM, CAT, [LABEL], cat_num).to(DEVICE)\n",
    "optimizer = torch.optim.SGD(the_model.parameters(), lr=0.001)\n",
    "\n",
    "# optimizer.step()\n",
    "data_count = 100\n",
    "# random pick data\n",
    "indices = torch.randperm(len(x))[:data_count]\n",
    "print(indices)\n",
    "train_data = x[indices]\n",
    "train_label = y[indices]\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = the_model(train_data[:data_count], epoch=200)\n",
    "    loss = torch.nn.functional.cross_entropy(output, train_label[:data_count])\n",
    "    loss.backward()\n",
    "    # print(((the_model.feature_importance_learners.grad).abs().max(dim=1)[0]))\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('-----------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(model, optimizer, datas, batch_size, epoch):\n",
    "    train_data, train_label, validation_data, validation_label = datas\n",
    "    \n",
    "    # slice data into batch\n",
    "    train_data = torch.split(train_data, batch_size)\n",
    "    train_label = torch.split(train_label, batch_size)\n",
    "\n",
    "    # losses and metrics\n",
    "    batch_loss = 0\n",
    "    train_acc = MulticlassAccuracy(num_classes=2).to(DEVICE)\n",
    "    train_auc = BinaryAUROC().to(DEVICE)\n",
    "    valid_acc = MulticlassAccuracy(num_classes=2).to(DEVICE)\n",
    "    valid_auc = BinaryAUROC().to(DEVICE)\n",
    "    \n",
    "    # train the model\n",
    "    stepper = trange(len(train_data))\n",
    "    for i in stepper:\n",
    "        stepper.set_description(f'Epoch {epoch}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_data[i], epoch=epoch)\n",
    "        loss = torch.nn.functional.cross_entropy(output, train_label[i]) * model.number_of_columns\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss += loss.item()\n",
    "        \n",
    "        # metrics\n",
    "        preds = output.softmax(dim=1)\n",
    "        true = torch.nn.functional.one_hot(train_label[i], num_classes=2).to(DEVICE)\n",
    "        train_acc.update(torch.argmax(preds, 1),true.T[1])\n",
    "        train_auc.update(preds.T[0],true.T[0])\n",
    "        \n",
    "        # at the end of epoch, print result and validate the model\n",
    "        if i == len(train_data) - 1:\n",
    "            train_acc = train_acc.compute()\n",
    "            train_auc = train_auc.compute()\n",
    "            stepper.set_postfix({'loss': round(batch_loss/(i+1), 3), 'acc': round(train_acc.item(), 3), 'AUC': round(train_auc.item(), 3)})\n",
    "            stepper.update()\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                output = model(validation_data, epoch=200)\n",
    "                # loss = torch.nn.functional.cross_entropy(output, validation_label[i])\n",
    "                preds = output.softmax(dim=1)\n",
    "                true = torch.nn.functional.one_hot(validation_label, num_classes=2).to(DEVICE)\n",
    "                valid_acc.update(torch.argmax(preds,1),true.T[1])\n",
    "                valid_auc.update(preds.T[0],true.T[0])\n",
    "            stepper.set_postfix({'loss': round(batch_loss/(i+1), 3), 'acc': round(train_acc.item(), 3), 'AUC': round(train_auc.item(), 3), 'val_acc': round(valid_acc.compute().item(), 3), 'val_AUC': round(valid_auc.compute().item(), 3)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_train(x, y):\n",
    "    # hyperparameter\n",
    "    epoch = 50\n",
    "    batch_size = 1024\n",
    "\n",
    "    \n",
    "    # shuffle data\n",
    "    indices = torch.randperm(len(x))\n",
    "    x = x[indices]\n",
    "    y = y[indices]\n",
    "    # slice data into train and test and validation\n",
    "    train_ratio = 0.7\n",
    "    validation_ratio = 0.1\n",
    "    train_data = x[:int(len(x)*train_ratio)]\n",
    "    train_label = y[:int(len(x)*train_ratio)]\n",
    "    validation_data = x[int(len(x)*train_ratio):int(len(x)*(train_ratio+validation_ratio))]\n",
    "    validation_label = y[int(len(x)*train_ratio):int(len(x)*(train_ratio+validation_ratio))]\n",
    "    test_data = x[int(len(x)*(train_ratio+validation_ratio)):]\n",
    "    test_label = y[int(len(x)*(train_ratio+validation_ratio)):]\n",
    "\n",
    "    # build model and optimizer\n",
    "    the_model = K_graph(NUM, CAT, [LABEL], cat_num).to(DEVICE)\n",
    "    optimizer = torch.optim.SGD(the_model.parameters(), lr=0.001)\n",
    "    \n",
    "    # train the model\n",
    "    datas = (train_data, train_label, validation_data, validation_label)\n",
    "    for i in range(epoch):\n",
    "        train_epoch(the_model, optimizer, datas, batch_size, epoch=i+1)\n",
    "    \n",
    "    # test the model\n",
    "    with torch.no_grad():\n",
    "        output = the_model(test_data, epoch=200)\n",
    "        preds = output.softmax(dim=1)\n",
    "        true = torch.nn.functional.one_hot(test_label, num_classes=2).to(DEVICE)\n",
    "        test_acc = MulticlassAccuracy(num_classes=2).to(DEVICE)\n",
    "        test_auc = BinaryAUROC().to(DEVICE)\n",
    "        test_acc.update(torch.argmax(preds,1),true.T[1])\n",
    "        test_auc.update(preds.T[0],true.T[0])\n",
    "        print('test_acc:', test_acc.compute().item())\n",
    "        print('test_auc:', test_auc.compute().item())\n",
    "        print('-----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 34/34 [00:00<00:00, 159.11it/s, loss=5.94, acc=0.809, AUC=0.819, val_acc=0.847, val_AUC=0.893]\n",
      "Epoch 2: 100%|██████████| 34/34 [00:00<00:00, 164.89it/s, loss=4.57, acc=0.849, AUC=0.902, val_acc=0.847, val_AUC=0.904]\n",
      "Epoch 3: 100%|██████████| 34/34 [00:00<00:00, 175.00it/s, loss=4.52, acc=0.85, AUC=0.904, val_acc=0.853, val_AUC=0.907]\n",
      "Epoch 4: 100%|██████████| 34/34 [00:00<00:00, 164.62it/s, loss=4.4, acc=0.854, AUC=0.909, val_acc=0.853, val_AUC=0.909]\n",
      "Epoch 5: 100%|██████████| 34/34 [00:00<00:00, 162.78it/s, loss=4.36, acc=0.855, AUC=0.91, val_acc=0.856, val_AUC=0.909]\n",
      "Epoch 6: 100%|██████████| 34/34 [00:00<00:00, 188.18it/s, loss=4.32, acc=0.857, AUC=0.912, val_acc=0.855, val_AUC=0.91]\n",
      "Epoch 7: 100%|██████████| 34/34 [00:00<00:00, 160.40it/s, loss=4.29, acc=0.857, AUC=0.913, val_acc=0.856, val_AUC=0.91]\n",
      "Epoch 8: 100%|██████████| 34/34 [00:00<00:00, 159.08it/s, loss=4.28, acc=0.858, AUC=0.914, val_acc=0.857, val_AUC=0.91]\n",
      "Epoch 9: 100%|██████████| 34/34 [00:00<00:00, 185.08it/s, loss=4.26, acc=0.859, AUC=0.914, val_acc=0.856, val_AUC=0.911]\n",
      "Epoch 10: 100%|██████████| 34/34 [00:00<00:00, 165.51it/s, loss=4.25, acc=0.859, AUC=0.915, val_acc=0.857, val_AUC=0.911]\n",
      "Epoch 11: 100%|██████████| 34/34 [00:00<00:00, 162.12it/s, loss=4.24, acc=0.86, AUC=0.915, val_acc=0.856, val_AUC=0.911]\n",
      "Epoch 12: 100%|██████████| 34/34 [00:00<00:00, 184.78it/s, loss=4.22, acc=0.86, AUC=0.916, val_acc=0.857, val_AUC=0.911]\n",
      "Epoch 13: 100%|██████████| 34/34 [00:00<00:00, 163.37it/s, loss=4.22, acc=0.86, AUC=0.916, val_acc=0.857, val_AUC=0.911]\n",
      "Epoch 14: 100%|██████████| 34/34 [00:00<00:00, 160.27it/s, loss=4.21, acc=0.86, AUC=0.916, val_acc=0.858, val_AUC=0.911]\n",
      "Epoch 15: 100%|██████████| 34/34 [00:00<00:00, 187.63it/s, loss=4.2, acc=0.861, AUC=0.917, val_acc=0.858, val_AUC=0.911]\n",
      "Epoch 16: 100%|██████████| 34/34 [00:00<00:00, 160.75it/s, loss=4.19, acc=0.862, AUC=0.917, val_acc=0.858, val_AUC=0.911]\n",
      "Epoch 17: 100%|██████████| 34/34 [00:00<00:00, 159.59it/s, loss=4.18, acc=0.862, AUC=0.917, val_acc=0.858, val_AUC=0.911]\n",
      "Epoch 18: 100%|██████████| 34/34 [00:00<00:00, 179.73it/s, loss=4.18, acc=0.862, AUC=0.918, val_acc=0.858, val_AUC=0.911]\n",
      "Epoch 19: 100%|██████████| 34/34 [00:00<00:00, 166.86it/s, loss=4.17, acc=0.862, AUC=0.918, val_acc=0.858, val_AUC=0.911]\n",
      "Epoch 20: 100%|██████████| 34/34 [00:00<00:00, 167.86it/s, loss=4.16, acc=0.862, AUC=0.918, val_acc=0.859, val_AUC=0.911]\n",
      "Epoch 21: 100%|██████████| 34/34 [00:00<00:00, 186.45it/s, loss=4.16, acc=0.862, AUC=0.918, val_acc=0.858, val_AUC=0.911]\n",
      "Epoch 22: 100%|██████████| 34/34 [00:00<00:00, 158.50it/s, loss=4.15, acc=0.863, AUC=0.919, val_acc=0.858, val_AUC=0.911]\n",
      "Epoch 23: 100%|██████████| 34/34 [00:00<00:00, 165.18it/s, loss=4.15, acc=0.863, AUC=0.919, val_acc=0.858, val_AUC=0.912]\n",
      "Epoch 24: 100%|██████████| 34/34 [00:00<00:00, 181.40it/s, loss=4.14, acc=0.863, AUC=0.919, val_acc=0.859, val_AUC=0.912]\n",
      "Epoch 25: 100%|██████████| 34/34 [00:00<00:00, 159.77it/s, loss=4.14, acc=0.863, AUC=0.919, val_acc=0.859, val_AUC=0.912]\n",
      "Epoch 26: 100%|██████████| 34/34 [00:00<00:00, 158.15it/s, loss=4.13, acc=0.863, AUC=0.919, val_acc=0.859, val_AUC=0.912]\n",
      "Epoch 27: 100%|██████████| 34/34 [00:00<00:00, 181.27it/s, loss=4.13, acc=0.863, AUC=0.92, val_acc=0.859, val_AUC=0.912]\n",
      "Epoch 28: 100%|██████████| 34/34 [00:00<00:00, 166.15it/s, loss=4.13, acc=0.863, AUC=0.92, val_acc=0.86, val_AUC=0.912]\n",
      "Epoch 29: 100%|██████████| 34/34 [00:00<00:00, 160.50it/s, loss=4.12, acc=0.863, AUC=0.92, val_acc=0.86, val_AUC=0.912]\n",
      "Epoch 30: 100%|██████████| 34/34 [00:00<00:00, 186.34it/s, loss=4.12, acc=0.863, AUC=0.92, val_acc=0.86, val_AUC=0.912]\n",
      "Epoch 31: 100%|██████████| 34/34 [00:00<00:00, 162.02it/s, loss=4.11, acc=0.863, AUC=0.92, val_acc=0.86, val_AUC=0.912]\n",
      "Epoch 32: 100%|██████████| 34/34 [00:00<00:00, 159.94it/s, loss=4.11, acc=0.863, AUC=0.92, val_acc=0.86, val_AUC=0.912]\n",
      "Epoch 33: 100%|██████████| 34/34 [00:00<00:00, 201.98it/s, loss=4.1, acc=0.864, AUC=0.921, val_acc=0.86, val_AUC=0.912]\n",
      "Epoch 34: 100%|██████████| 34/34 [00:00<00:00, 189.07it/s, loss=4.1, acc=0.864, AUC=0.921, val_acc=0.859, val_AUC=0.912]\n",
      "Epoch 35: 100%|██████████| 34/34 [00:00<00:00, 191.64it/s, loss=4.1, acc=0.864, AUC=0.921, val_acc=0.859, val_AUC=0.912]\n",
      "Epoch 36: 100%|██████████| 34/34 [00:00<00:00, 188.72it/s, loss=4.09, acc=0.864, AUC=0.921, val_acc=0.859, val_AUC=0.912]\n",
      "Epoch 37: 100%|██████████| 34/34 [00:00<00:00, 188.75it/s, loss=4.09, acc=0.865, AUC=0.921, val_acc=0.859, val_AUC=0.912]\n",
      "Epoch 38: 100%|██████████| 34/34 [00:00<00:00, 188.95it/s, loss=4.08, acc=0.865, AUC=0.921, val_acc=0.859, val_AUC=0.912]\n",
      "Epoch 39: 100%|██████████| 34/34 [00:00<00:00, 176.41it/s, loss=4.08, acc=0.865, AUC=0.922, val_acc=0.859, val_AUC=0.912]\n",
      "Epoch 40: 100%|██████████| 34/34 [00:00<00:00, 172.56it/s, loss=4.08, acc=0.865, AUC=0.922, val_acc=0.858, val_AUC=0.912]\n",
      "Epoch 41: 100%|██████████| 34/34 [00:00<00:00, 156.84it/s, loss=4.07, acc=0.866, AUC=0.922, val_acc=0.859, val_AUC=0.912]\n",
      "Epoch 42: 100%|██████████| 34/34 [00:00<00:00, 171.81it/s, loss=4.07, acc=0.866, AUC=0.922, val_acc=0.859, val_AUC=0.912]\n",
      "Epoch 43: 100%|██████████| 34/34 [00:00<00:00, 180.15it/s, loss=4.07, acc=0.865, AUC=0.922, val_acc=0.859, val_AUC=0.912]\n",
      "Epoch 44: 100%|██████████| 34/34 [00:00<00:00, 157.53it/s, loss=4.06, acc=0.866, AUC=0.923, val_acc=0.858, val_AUC=0.912]\n",
      "Epoch 45: 100%|██████████| 34/34 [00:00<00:00, 164.06it/s, loss=4.06, acc=0.866, AUC=0.922, val_acc=0.858, val_AUC=0.912]\n",
      "Epoch 46: 100%|██████████| 34/34 [00:00<00:00, 187.67it/s, loss=4.05, acc=0.866, AUC=0.923, val_acc=0.858, val_AUC=0.912]\n",
      "Epoch 47: 100%|██████████| 34/34 [00:00<00:00, 157.28it/s, loss=4.05, acc=0.866, AUC=0.923, val_acc=0.858, val_AUC=0.912]\n",
      "Epoch 48: 100%|██████████| 34/34 [00:00<00:00, 154.75it/s, loss=4.04, acc=0.866, AUC=0.923, val_acc=0.857, val_AUC=0.912]\n",
      "Epoch 49: 100%|██████████| 34/34 [00:00<00:00, 168.01it/s, loss=4.04, acc=0.866, AUC=0.923, val_acc=0.858, val_AUC=0.912]\n",
      "Epoch 50: 100%|██████████| 34/34 [00:00<00:00, 172.65it/s, loss=4.03, acc=0.867, AUC=0.923, val_acc=0.858, val_AUC=0.912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.8497287631034851\n",
      "test_auc: 0.9133439756728756\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "overall_train(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8547446131706238"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([0.8570989966392517,\n",
    "     0.8552564382553101,\n",
    "     0.857201337814331,\n",
    "     0.854437530040741,\n",
    "     0.8497287631034851,\n",
    "     ])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9127093376609399"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([0.9126779092790669,\n",
    "     0.91077120905494,\n",
    "     0.9158374102217134,\n",
    "     0.9109161840761032,\n",
    "     0.9133439756728756])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
