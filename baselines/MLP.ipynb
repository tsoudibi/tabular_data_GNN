{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import torch_geometric\n",
    "from tqdm import tqdm, trange\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from torcheval.metrics import BinaryAUROC\n",
    "import numpy as np\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# DEVICE = torch.device('cuda')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48842, 14]) torch.Size([48842])\n",
      "[9, 16, 7, 15, 6, 5, 2, 42]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.995129</td>\n",
       "      <td>0.351675</td>\n",
       "      <td>-1.197259</td>\n",
       "      <td>-0.144804</td>\n",
       "      <td>-0.217127</td>\n",
       "      <td>-0.034087</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.046942</td>\n",
       "      <td>-0.945524</td>\n",
       "      <td>-0.419335</td>\n",
       "      <td>-0.144804</td>\n",
       "      <td>-0.217127</td>\n",
       "      <td>0.772930</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.776316</td>\n",
       "      <td>1.394723</td>\n",
       "      <td>0.747550</td>\n",
       "      <td>-0.144804</td>\n",
       "      <td>-0.217127</td>\n",
       "      <td>-0.034087</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.390683</td>\n",
       "      <td>-0.277844</td>\n",
       "      <td>-0.030373</td>\n",
       "      <td>0.886874</td>\n",
       "      <td>-0.217127</td>\n",
       "      <td>-0.034087</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.505691</td>\n",
       "      <td>-0.815954</td>\n",
       "      <td>-0.030373</td>\n",
       "      <td>-0.144804</td>\n",
       "      <td>-0.217127</td>\n",
       "      <td>-0.841104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>-0.849254</td>\n",
       "      <td>0.640492</td>\n",
       "      <td>0.747550</td>\n",
       "      <td>-0.144804</td>\n",
       "      <td>-0.217127</td>\n",
       "      <td>-0.195490</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>0.098933</td>\n",
       "      <td>-0.334178</td>\n",
       "      <td>-0.419335</td>\n",
       "      <td>-0.144804</td>\n",
       "      <td>-0.217127</td>\n",
       "      <td>-0.034087</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>1.411808</td>\n",
       "      <td>-0.357510</td>\n",
       "      <td>-0.419335</td>\n",
       "      <td>-0.144804</td>\n",
       "      <td>-0.217127</td>\n",
       "      <td>-0.034087</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>-1.213941</td>\n",
       "      <td>0.111984</td>\n",
       "      <td>-0.419335</td>\n",
       "      <td>-0.144804</td>\n",
       "      <td>-0.217127</td>\n",
       "      <td>-1.648120</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>0.974183</td>\n",
       "      <td>0.930494</td>\n",
       "      <td>-0.419335</td>\n",
       "      <td>1.871315</td>\n",
       "      <td>-0.217127</td>\n",
       "      <td>-0.034087</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48842 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age    fnlwgt  educational-num  capital-gain  capital-loss  \\\n",
       "0     -0.995129  0.351675        -1.197259     -0.144804     -0.217127   \n",
       "1     -0.046942 -0.945524        -0.419335     -0.144804     -0.217127   \n",
       "2     -0.776316  1.394723         0.747550     -0.144804     -0.217127   \n",
       "3      0.390683 -0.277844        -0.030373      0.886874     -0.217127   \n",
       "4     -1.505691 -0.815954        -0.030373     -0.144804     -0.217127   \n",
       "...         ...       ...              ...           ...           ...   \n",
       "48837 -0.849254  0.640492         0.747550     -0.144804     -0.217127   \n",
       "48838  0.098933 -0.334178        -0.419335     -0.144804     -0.217127   \n",
       "48839  1.411808 -0.357510        -0.419335     -0.144804     -0.217127   \n",
       "48840 -1.213941  0.111984        -0.419335     -0.144804     -0.217127   \n",
       "48841  0.974183  0.930494        -0.419335      1.871315     -0.217127   \n",
       "\n",
       "       hours-per-week  workclass  education  marital-status  occupation  \\\n",
       "0           -0.034087        4.0        1.0             4.0         7.0   \n",
       "1            0.772930        4.0       11.0             2.0         5.0   \n",
       "2           -0.034087        2.0        7.0             2.0        11.0   \n",
       "3           -0.034087        4.0       15.0             2.0         7.0   \n",
       "4           -0.841104        0.0       15.0             4.0         0.0   \n",
       "...               ...        ...        ...             ...         ...   \n",
       "48837       -0.195490        4.0        7.0             2.0        13.0   \n",
       "48838       -0.034087        4.0       11.0             2.0         7.0   \n",
       "48839       -0.034087        4.0       11.0             6.0         1.0   \n",
       "48840       -1.648120        4.0       11.0             4.0         1.0   \n",
       "48841       -0.034087        5.0       11.0             2.0         4.0   \n",
       "\n",
       "       relationship  race  gender  native-country  income  \n",
       "0               3.0   2.0     1.0            39.0       0  \n",
       "1               0.0   4.0     1.0            39.0       0  \n",
       "2               0.0   4.0     1.0            39.0       1  \n",
       "3               0.0   2.0     1.0            39.0       1  \n",
       "4               3.0   4.0     0.0            39.0       0  \n",
       "...             ...   ...     ...             ...     ...  \n",
       "48837           5.0   4.0     0.0            39.0       0  \n",
       "48838           0.0   4.0     1.0            39.0       1  \n",
       "48839           4.0   4.0     0.0            39.0       0  \n",
       "48840           3.0   4.0     1.0            39.0       0  \n",
       "48841           5.0   4.0     0.0            39.0       1  \n",
       "\n",
       "[48842 rows x 15 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get feature from csv\n",
    "RAW_data = pd.read_csv('../data/adult.csv')\n",
    "CAT = ['workclass','education','marital-status','occupation','relationship','race','gender','native-country']\n",
    "NUM = ['age','fnlwgt','educational-num','capital-gain','capital-loss','hours-per-week']\n",
    "LABEL = 'income'\n",
    "# convert categorical data to ordinal data\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "enc = OrdinalEncoder()\n",
    "data_pd = RAW_data.copy()\n",
    "data_pd[CAT] = enc.fit_transform(RAW_data[CAT])\n",
    "# data_pd = pd.get_dummies(RAW_data, columns=CAT, dtype=float)\n",
    "# label to category\n",
    "data_pd[LABEL] = data_pd[LABEL].astype('category').cat.codes\n",
    "\n",
    "# realign data to num + cat\n",
    "data_pd = data_pd[NUM + CAT + [LABEL]]\n",
    "\n",
    "# caculate unique value of each categorical feature\n",
    "cat_num = [len(data_pd[col].unique()) for col in CAT]\n",
    "\n",
    "# normalize numerical data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_pd[NUM] = scaler.fit_transform(data_pd[NUM])\n",
    "\n",
    "# convert data to tensor\n",
    "x = torch.tensor(data_pd.drop(columns=[LABEL]).values, dtype=torch.float, device=DEVICE)  # [48842, 108]\n",
    "y = torch.tensor(data_pd[LABEL].values, dtype=torch.long, device=DEVICE) # [48842]\n",
    "print(x.shape, y.shape)\n",
    "print(cat_num)\n",
    "data_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_graph(torch.nn.Module):\n",
    "    def __init__(self, NUM, CAT, LABEL, cat_num):\n",
    "        super(K_graph, self).__init__()\n",
    "        '''\n",
    "        num_cols: number of numerical columns\n",
    "        cat_cols: number of categorical columns\n",
    "        label_cols: number of label columns\n",
    "        cat_num: number of unique value of each categorical columns\n",
    "        '''\n",
    "        self.hidden_dim = 128\n",
    "        \n",
    "        # order: num -> cat -> label\n",
    "        self.num_cols = len(NUM)\n",
    "        self.cat_cols = len(CAT)\n",
    "        self.label_cols = len(LABEL)\n",
    "        self.number_of_columns = self.num_cols + self.cat_cols \n",
    "        \n",
    "        \n",
    "        # numerical feature\n",
    "        self.num_embeddings = torch.nn.ModuleList([torch.nn.Linear(1, self.hidden_dim) for i in range(self.num_cols)])\n",
    "        # categorical feature\n",
    "        self.cat_embeddings = torch.nn.ModuleList([torch.nn.Embedding(cat_num[i], self.hidden_dim) for i in range(self.cat_cols)])\n",
    "        \n",
    "        self.prediction = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.hidden_dim * self.number_of_columns, self.hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.LayerNorm(self.hidden_dim),\n",
    "            torch.nn.Linear(self.hidden_dim, self.label_cols + 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_data, epoch = -1):\n",
    "        \n",
    "        # make feature embedding\n",
    "        num_data = input_data[:,:self.num_cols].unsqueeze(-1).unsqueeze(-1) \n",
    "        feature_embedding_num = torch.cat([self.num_embeddings[i](num_data[:,i]) for i in range(self.num_cols)], dim=1).reshape(len(input_data), -1) # [batch_size, num_cols * hidden_dim]\n",
    "        feature_embedding_num = torch.nn.ReLU()(feature_embedding_num)\n",
    "        feature_embedding_num = torch.layer_norm(feature_embedding_num, feature_embedding_num.shape)\n",
    "        # categorical feature\n",
    "        feature_embedding_cat = torch.cat([self.cat_embeddings[i](input_data[:,self.num_cols+i].long()) for i in range(self.cat_cols)], dim=1) # [batch_size, cat_cols * hidden_dim]\n",
    "        feature_embedding_cat = torch.layer_norm(feature_embedding_cat, feature_embedding_cat.shape)\n",
    "        # concat\n",
    "        feature_embedding = torch.cat((feature_embedding_num, feature_embedding_cat), dim=1)\n",
    "        \n",
    "        \n",
    "        # make prediction\n",
    "        prediction = self.prediction(feature_embedding)\n",
    "        \n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22371,  5364, 37202, 44811, 26775,  7994, 34834, 47511, 22717, 16393,\n",
      "        26122, 28500, 27572,  8844,  1484, 30446, 13540, 37981, 28718, 46577,\n",
      "        15428, 26049,  9585,  6987, 39265, 29731, 30747, 16554, 41138, 30885,\n",
      "        23852,  6845, 29424, 24103, 23834, 22568, 29320, 34425,  1772,   574,\n",
      "        28401, 48717,   448, 46434, 20371,  9352,  5733, 10675, 11936, 26987,\n",
      "        28656, 40510, 26226, 42957, 37303,  4911,   586,  8843, 48665, 27667,\n",
      "        16934, 35946, 35680, 17880, 47046, 26828, 19523, 25069, 40836, 28897,\n",
      "        34122, 30370, 39037, 43368, 32417,  6773,  2899, 46399, 45489, 19374,\n",
      "          960,  6647, 38570, 17485, 28797, 40258,  7077, 13196,  1120, 22257,\n",
      "         1132, 22436, 36056,    86, 39565, 10195,  3517, 11279, 46490,   387])\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "the_model = K_graph(NUM, CAT, [LABEL], cat_num).to(DEVICE)\n",
    "optimizer = torch.optim.SGD(the_model.parameters(), lr=0.001)\n",
    "\n",
    "# optimizer.step()\n",
    "data_count = 100\n",
    "# random pick data\n",
    "indices = torch.randperm(len(x))[:data_count]\n",
    "print(indices)\n",
    "train_data = x[indices]\n",
    "train_label = y[indices]\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = the_model(train_data[:data_count], epoch=200)\n",
    "    loss = torch.nn.functional.cross_entropy(output, train_label[:data_count])\n",
    "    loss.backward()\n",
    "    # print(((the_model.feature_importance_learners.grad).abs().max(dim=1)[0]))\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('-----------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(model, optimizer, datas, batch_size, epoch):\n",
    "    train_data, train_label, validation_data, validation_label = datas\n",
    "    \n",
    "    # slice data into batch\n",
    "    train_data = torch.split(train_data, batch_size)\n",
    "    train_label = torch.split(train_label, batch_size)\n",
    "\n",
    "    # losses and metrics\n",
    "    batch_loss = 0\n",
    "    train_acc = MulticlassAccuracy(num_classes=2).to(DEVICE)\n",
    "    train_auc = BinaryAUROC().to(DEVICE)\n",
    "    valid_acc = MulticlassAccuracy(num_classes=2).to(DEVICE)\n",
    "    valid_auc = BinaryAUROC().to(DEVICE)\n",
    "    \n",
    "    # train the model\n",
    "    stepper = trange(len(train_data))\n",
    "    for i in stepper:\n",
    "        stepper.set_description(f'Epoch {epoch}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_data[i], epoch=epoch)\n",
    "        loss = torch.nn.functional.cross_entropy(output, train_label[i]) * model.number_of_columns\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss += loss.item()\n",
    "        \n",
    "        # metrics\n",
    "        preds = output.softmax(dim=1)\n",
    "        true = torch.nn.functional.one_hot(train_label[i], num_classes=2).to(DEVICE)\n",
    "        train_acc.update(torch.argmax(preds, 1),true.T[1])\n",
    "        train_auc.update(preds.T[0],true.T[0])\n",
    "        \n",
    "        # at the end of epoch, print result and validate the model\n",
    "        if i == len(train_data) - 1:\n",
    "            train_acc = train_acc.compute()\n",
    "            train_auc = train_auc.compute()\n",
    "            stepper.set_postfix({'loss': round(batch_loss/(i+1), 3), 'acc': round(train_acc.item(), 3), 'AUC': round(train_auc.item(), 3)})\n",
    "            stepper.update()\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                output = model(validation_data, epoch=200)\n",
    "                # loss = torch.nn.functional.cross_entropy(output, validation_label[i])\n",
    "                preds = output.softmax(dim=1)\n",
    "                true = torch.nn.functional.one_hot(validation_label, num_classes=2).to(DEVICE)\n",
    "                valid_acc.update(torch.argmax(preds,1),true.T[1])\n",
    "                valid_auc.update(preds.T[0],true.T[0])\n",
    "            stepper.set_postfix({'loss': round(batch_loss/(i+1), 3), 'acc': round(train_acc.item(), 3), 'AUC': round(train_auc.item(), 3), 'val_acc': round(valid_acc.compute().item(), 3), 'val_AUC': round(valid_auc.compute().item(), 3)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_train(x, y):\n",
    "    # hyperparameter\n",
    "    epoch = 50\n",
    "    batch_size = 1024\n",
    "\n",
    "    \n",
    "    # shuffle data\n",
    "    indices = torch.randperm(len(x))\n",
    "    x = x[indices]\n",
    "    y = y[indices]\n",
    "    # slice data into train and test and validation\n",
    "    train_ratio = 0.7\n",
    "    validation_ratio = 0.1\n",
    "    train_data = x[:int(len(x)*train_ratio)]\n",
    "    train_label = y[:int(len(x)*train_ratio)]\n",
    "    validation_data = x[int(len(x)*train_ratio):int(len(x)*(train_ratio+validation_ratio))]\n",
    "    validation_label = y[int(len(x)*train_ratio):int(len(x)*(train_ratio+validation_ratio))]\n",
    "    test_data = x[int(len(x)*(train_ratio+validation_ratio)):]\n",
    "    test_label = y[int(len(x)*(train_ratio+validation_ratio)):]\n",
    "\n",
    "    # build model and optimizer\n",
    "    the_model = K_graph(NUM, CAT, [LABEL], cat_num).to(DEVICE)\n",
    "    optimizer = torch.optim.SGD(the_model.parameters(), lr=0.001)\n",
    "    \n",
    "    # train the model\n",
    "    datas = (train_data, train_label, validation_data, validation_label)\n",
    "    for i in range(epoch):\n",
    "        train_epoch(the_model, optimizer, datas, batch_size, epoch=i+1)\n",
    "    \n",
    "    # test the model\n",
    "    with torch.no_grad():\n",
    "        test_data = torch.split(test_data, batch_size)\n",
    "        test_label = torch.split(test_label, batch_size)\n",
    "        for i in range(len(test_data)):\n",
    "            output = the_model(test_data[i], epoch=200)\n",
    "            preds = output.softmax(dim=1)\n",
    "            true = torch.nn.functional.one_hot(test_label[i], num_classes=2).to(DEVICE)\n",
    "            test_acc = MulticlassAccuracy(num_classes=2).to(DEVICE)\n",
    "            test_auc = BinaryAUROC().to(DEVICE)\n",
    "            test_acc.update(torch.argmax(preds,1),true.T[1])\n",
    "            test_auc.update(preds.T[0],true.T[0])\n",
    "\n",
    "        print('test_acc:', test_acc.compute().item())\n",
    "        print('test_auc:', test_auc.compute().item())\n",
    "        print('-----------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 34/34 [00:00<00:00, 197.36it/s, loss=5.27, acc=0.824, AUC=0.862, val_acc=0.847, val_AUC=0.901]\n",
      "Epoch 2:   0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 34/34 [00:00<00:00, 211.60it/s, loss=4.59, acc=0.848, AUC=0.9, val_acc=0.847, val_AUC=0.907]\n",
      "Epoch 3: 100%|██████████| 34/34 [00:00<00:00, 214.37it/s, loss=4.46, acc=0.853, AUC=0.906, val_acc=0.85, val_AUC=0.909]\n",
      "Epoch 4: 100%|██████████| 34/34 [00:00<00:00, 214.98it/s, loss=4.4, acc=0.855, AUC=0.909, val_acc=0.851, val_AUC=0.91]\n",
      "Epoch 5: 100%|██████████| 34/34 [00:00<00:00, 216.35it/s, loss=4.37, acc=0.856, AUC=0.91, val_acc=0.854, val_AUC=0.911]\n",
      "Epoch 6: 100%|██████████| 34/34 [00:00<00:00, 214.59it/s, loss=4.34, acc=0.857, AUC=0.911, val_acc=0.853, val_AUC=0.911]\n",
      "Epoch 7: 100%|██████████| 34/34 [00:00<00:00, 215.46it/s, loss=4.32, acc=0.857, AUC=0.912, val_acc=0.854, val_AUC=0.912]\n",
      "Epoch 8: 100%|██████████| 34/34 [00:00<00:00, 214.94it/s, loss=4.3, acc=0.858, AUC=0.913, val_acc=0.854, val_AUC=0.912]\n",
      "Epoch 9: 100%|██████████| 34/34 [00:00<00:00, 215.19it/s, loss=4.28, acc=0.859, AUC=0.914, val_acc=0.854, val_AUC=0.912]\n",
      "Epoch 10: 100%|██████████| 34/34 [00:00<00:00, 215.51it/s, loss=4.27, acc=0.859, AUC=0.914, val_acc=0.854, val_AUC=0.912]\n",
      "Epoch 11: 100%|██████████| 34/34 [00:00<00:00, 215.23it/s, loss=4.26, acc=0.859, AUC=0.915, val_acc=0.855, val_AUC=0.913]\n",
      "Epoch 12: 100%|██████████| 34/34 [00:00<00:00, 215.45it/s, loss=4.25, acc=0.86, AUC=0.915, val_acc=0.855, val_AUC=0.913]\n",
      "Epoch 13: 100%|██████████| 34/34 [00:00<00:00, 215.91it/s, loss=4.24, acc=0.86, AUC=0.916, val_acc=0.855, val_AUC=0.913]\n",
      "Epoch 14: 100%|██████████| 34/34 [00:00<00:00, 216.44it/s, loss=4.23, acc=0.861, AUC=0.916, val_acc=0.855, val_AUC=0.913]\n",
      "Epoch 15: 100%|██████████| 34/34 [00:00<00:00, 215.16it/s, loss=4.22, acc=0.861, AUC=0.916, val_acc=0.856, val_AUC=0.913]\n",
      "Epoch 16: 100%|██████████| 34/34 [00:00<00:00, 213.76it/s, loss=4.21, acc=0.861, AUC=0.917, val_acc=0.856, val_AUC=0.913]\n",
      "Epoch 17: 100%|██████████| 34/34 [00:00<00:00, 214.87it/s, loss=4.21, acc=0.861, AUC=0.917, val_acc=0.856, val_AUC=0.914]\n",
      "Epoch 18: 100%|██████████| 34/34 [00:00<00:00, 214.69it/s, loss=4.2, acc=0.862, AUC=0.917, val_acc=0.856, val_AUC=0.914]\n",
      "Epoch 19: 100%|██████████| 34/34 [00:00<00:00, 215.01it/s, loss=4.19, acc=0.862, AUC=0.918, val_acc=0.855, val_AUC=0.914]\n",
      "Epoch 20: 100%|██████████| 34/34 [00:00<00:00, 215.20it/s, loss=4.18, acc=0.862, AUC=0.918, val_acc=0.855, val_AUC=0.914]\n",
      "Epoch 21: 100%|██████████| 34/34 [00:00<00:00, 214.42it/s, loss=4.18, acc=0.863, AUC=0.918, val_acc=0.855, val_AUC=0.914]\n",
      "Epoch 22: 100%|██████████| 34/34 [00:00<00:00, 215.48it/s, loss=4.17, acc=0.863, AUC=0.918, val_acc=0.854, val_AUC=0.914]\n",
      "Epoch 23: 100%|██████████| 34/34 [00:00<00:00, 215.03it/s, loss=4.16, acc=0.863, AUC=0.919, val_acc=0.855, val_AUC=0.914]\n",
      "Epoch 24: 100%|██████████| 34/34 [00:00<00:00, 215.25it/s, loss=4.16, acc=0.863, AUC=0.919, val_acc=0.856, val_AUC=0.914]\n",
      "Epoch 25: 100%|██████████| 34/34 [00:00<00:00, 214.70it/s, loss=4.15, acc=0.863, AUC=0.919, val_acc=0.855, val_AUC=0.914]\n",
      "Epoch 26: 100%|██████████| 34/34 [00:00<00:00, 215.66it/s, loss=4.14, acc=0.864, AUC=0.919, val_acc=0.855, val_AUC=0.914]\n",
      "Epoch 27: 100%|██████████| 34/34 [00:00<00:00, 214.87it/s, loss=4.14, acc=0.864, AUC=0.92, val_acc=0.855, val_AUC=0.914]\n",
      "Epoch 28: 100%|██████████| 34/34 [00:00<00:00, 215.21it/s, loss=4.13, acc=0.864, AUC=0.92, val_acc=0.854, val_AUC=0.914]\n",
      "Epoch 29: 100%|██████████| 34/34 [00:00<00:00, 215.01it/s, loss=4.13, acc=0.864, AUC=0.92, val_acc=0.855, val_AUC=0.914]\n",
      "Epoch 30: 100%|██████████| 34/34 [00:00<00:00, 215.38it/s, loss=4.12, acc=0.864, AUC=0.92, val_acc=0.855, val_AUC=0.914]\n",
      "Epoch 31: 100%|██████████| 34/34 [00:00<00:00, 214.12it/s, loss=4.12, acc=0.864, AUC=0.92, val_acc=0.854, val_AUC=0.914]\n",
      "Epoch 32: 100%|██████████| 34/34 [00:00<00:00, 214.62it/s, loss=4.11, acc=0.865, AUC=0.921, val_acc=0.853, val_AUC=0.914]\n",
      "Epoch 33: 100%|██████████| 34/34 [00:00<00:00, 215.20it/s, loss=4.11, acc=0.865, AUC=0.921, val_acc=0.853, val_AUC=0.914]\n",
      "Epoch 34: 100%|██████████| 34/34 [00:00<00:00, 214.55it/s, loss=4.1, acc=0.865, AUC=0.921, val_acc=0.853, val_AUC=0.914]\n",
      "Epoch 35: 100%|██████████| 34/34 [00:00<00:00, 214.28it/s, loss=4.1, acc=0.866, AUC=0.921, val_acc=0.854, val_AUC=0.914]\n",
      "Epoch 36: 100%|██████████| 34/34 [00:00<00:00, 210.66it/s, loss=4.09, acc=0.866, AUC=0.921, val_acc=0.854, val_AUC=0.914]\n",
      "Epoch 37: 100%|██████████| 34/34 [00:00<00:00, 215.64it/s, loss=4.09, acc=0.866, AUC=0.922, val_acc=0.854, val_AUC=0.914]\n",
      "Epoch 38: 100%|██████████| 34/34 [00:00<00:00, 215.95it/s, loss=4.08, acc=0.866, AUC=0.922, val_acc=0.854, val_AUC=0.914]\n",
      "Epoch 39: 100%|██████████| 34/34 [00:00<00:00, 216.25it/s, loss=4.08, acc=0.866, AUC=0.922, val_acc=0.854, val_AUC=0.914]\n",
      "Epoch 40: 100%|██████████| 34/34 [00:00<00:00, 216.82it/s, loss=4.07, acc=0.867, AUC=0.922, val_acc=0.854, val_AUC=0.914]\n",
      "Epoch 41: 100%|██████████| 34/34 [00:00<00:00, 216.34it/s, loss=4.07, acc=0.867, AUC=0.923, val_acc=0.854, val_AUC=0.914]\n",
      "Epoch 42: 100%|██████████| 34/34 [00:00<00:00, 215.90it/s, loss=4.06, acc=0.867, AUC=0.923, val_acc=0.854, val_AUC=0.914]\n",
      "Epoch 43: 100%|██████████| 34/34 [00:00<00:00, 214.98it/s, loss=4.06, acc=0.867, AUC=0.923, val_acc=0.853, val_AUC=0.915]\n",
      "Epoch 44: 100%|██████████| 34/34 [00:00<00:00, 214.38it/s, loss=4.05, acc=0.867, AUC=0.923, val_acc=0.854, val_AUC=0.915]\n",
      "Epoch 45: 100%|██████████| 34/34 [00:00<00:00, 215.12it/s, loss=4.04, acc=0.867, AUC=0.923, val_acc=0.854, val_AUC=0.914]\n",
      "Epoch 46: 100%|██████████| 34/34 [00:00<00:00, 215.29it/s, loss=4.04, acc=0.868, AUC=0.923, val_acc=0.854, val_AUC=0.914]\n",
      "Epoch 47: 100%|██████████| 34/34 [00:00<00:00, 213.92it/s, loss=4.04, acc=0.868, AUC=0.924, val_acc=0.855, val_AUC=0.914]\n",
      "Epoch 48: 100%|██████████| 34/34 [00:00<00:00, 214.24it/s, loss=4.04, acc=0.867, AUC=0.924, val_acc=0.854, val_AUC=0.914]\n",
      "Epoch 49: 100%|██████████| 34/34 [00:00<00:00, 215.19it/s, loss=4.03, acc=0.867, AUC=0.924, val_acc=0.854, val_AUC=0.914]\n",
      "Epoch 50: 100%|██████████| 34/34 [00:00<00:00, 215.43it/s, loss=4.02, acc=0.868, AUC=0.924, val_acc=0.855, val_AUC=0.914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.8264014720916748\n",
      "test_auc: 0.8963480128893663\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "overall_train(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8524412274360657"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([0.8390596508979797,\n",
    "     0.8734177350997925,\n",
    "     0.8661844730377197,\n",
    "     0.8444846272468567,\n",
    "     0.8390596508979797\n",
    "     ])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9185009052376565"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在目前儲存格或上一個儲存格中執行程式碼時，Kernel 已損毀。請檢閱儲存格中的程式碼，找出失敗的可能原因。如需詳細資訊，請按一下<a href='https://aka.ms/vscodeJupyterKernelCrash'>這裡</a>。檢視 Jupyter <a href='command:jupyter.viewOutput'>記錄</a>，以取得進一步的詳細資料。"
     ]
    }
   ],
   "source": [
    "sum([0.911195810921526,\n",
    "     0.939216728690413,\n",
    "     0.9200849868064837,\n",
    "     0.9178027955656545,\n",
    "     0.9042042042042042\n",
    "     ])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
