{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import torch_geometric\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# DEVICE = torch.device('cuda')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature from csv\n",
    "RAW_data = pd.read_csv('data/adult.csv')\n",
    "CAT = ['workclass','education','marital-status','occupation','relationship','race','gender','native-country']\n",
    "NUM = ['age','fnlwgt','educational-num','capital-gain','capital-loss','hours-per-week']\n",
    "LABEL = 'income'\n",
    "# convert categorical data to ordinal data\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "enc = OrdinalEncoder()\n",
    "data_pd = RAW_data.copy()\n",
    "data_pd[CAT] = enc.fit_transform(RAW_data[CAT])\n",
    "# data_pd = pd.get_dummies(RAW_data, columns=CAT, dtype=float)\n",
    "# label to category\n",
    "data_pd[LABEL] = data_pd[LABEL].astype('category').cat.codes\n",
    "\n",
    "# realign data to num + cat\n",
    "data_pd = data_pd[NUM + CAT + [LABEL]]\n",
    "\n",
    "# caculate unique value of each categorical feature\n",
    "cat_num = [len(data_pd[col].unique()) for col in CAT]\n",
    "\n",
    "# normalize numerical data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_pd[NUM] = scaler.fit_transform(data_pd[NUM])\n",
    "\n",
    "# convert data to tensor\n",
    "x = torch.tensor(data_pd.drop(columns=[LABEL]).values, dtype=torch.float, device=DEVICE)  # [48842, 108]\n",
    "y = torch.tensor(data_pd[LABEL].values, dtype=torch.long, device=DEVICE) # [48842]\n",
    "print(x.shape, y.shape)\n",
    "print(cat_num)\n",
    "data_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make column node embedding\n",
    "columns = torch.tensor(range(len(RAW_data.columns.values)-1), dtype=torch.long, device=DEVICE) # [14]\n",
    "print(columns)\n",
    "columns_embedder = torch.nn.Embedding(len(RAW_data.columns.values)-1, 108, device=DEVICE) # [14, 108]\n",
    "columns_emb = columns_embedder(columns) #[14, 108]\n",
    "print(columns_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_graph(torch.nn.Module):\n",
    "    def __init__(self, NUM, CAT, LABEL, cat_num):\n",
    "        super(K_graph, self).__init__()\n",
    "        '''\n",
    "        num_cols: number of numerical columns\n",
    "        cat_cols: number of categorical columns\n",
    "        label_cols: number of label columns\n",
    "        cat_num: number of unique value of each categorical columns\n",
    "        '''\n",
    "        self.hidden_dim = 8\n",
    "        \n",
    "        # order: num -> cat -> label\n",
    "        self.num_cols = len(NUM)\n",
    "        self.cat_cols = len(CAT)\n",
    "        self.label_cols = len(LABEL)\n",
    "        self.number_of_columns = self.num_cols + self.cat_cols \n",
    "        \n",
    "        # layers\n",
    "        # self.feature_importance_learner = torch.nn.Sequential(\n",
    "        #     torch.nn.Linear(self.hidden_dim * (self.number_of_columns), self.number_of_columns),\n",
    "        #     torch.nn.Sigmoid(),\n",
    "        # )\n",
    "\n",
    "        # 請看下方的playground\n",
    "        # self.feature_importance_learners_parameter = torch.nn.Parameter(torch.randn(self.number_of_columns, self.hidden_dim, device=DEVICE), requires_grad=True) # [hidden_dim, cols]\n",
    "        # target = torch.nn.Parameter(torch.zeros(((self.number_of_columns, self.hidden_dim * self.number_of_columns)), device=DEVICE, requires_grad=True))\n",
    "        # # target = torch.zeros(((self.number_of_columns, self.hidden_dim * self.number_of_columns)), device=DEVICE, requires_grad=True)\n",
    "        # result = target.clone()\n",
    "        # index = torch.arange(self.hidden_dim * self.number_of_columns, device=DEVICE).view(self.number_of_columns, self.hidden_dim)\n",
    "        # # print(result.shape)\n",
    "        # # print(index.shape)\n",
    "        # # print(self.feature_importance_learners_parameter.shape)\n",
    "        # result.scatter_add_(-1, index, self.feature_importance_learners_parameter.float())\n",
    "        # target = result.detach().requires_grad_()\n",
    "        # # self.feature_importance_learners = target.T # [(cols*hidden_dim), hidden_dim]\n",
    "        # self.feature_importance_learners = target.T # [(cols*hidden_dim), hidden_dim]\n",
    "        # xavier init\n",
    "        self.feature_importance_learners = torch.nn.Parameter(torch.zeros(self.number_of_columns, self.hidden_dim * self.number_of_columns, device=DEVICE), requires_grad=True) # [hidden_dim * cols, cols]\n",
    "        # self.feature_importance_learners = self.feature_importance_learners + 1/self.hidden_dim\n",
    "        index = torch.arange(self.hidden_dim * self.number_of_columns, device=DEVICE).view(self.number_of_columns, self.hidden_dim)\n",
    "        self.feature_importance_learners_mask = torch.zeros(self.number_of_columns, self.hidden_dim * self.number_of_columns, device=DEVICE).scatter_(1, index, torch.ones((self.number_of_columns, self.hidden_dim),device=DEVICE)).bool()\n",
    "        # self.feature_importance_learners_bias = torch.nn.Parameter(torch.randn(self.number_of_columns, device=DEVICE), requires_grad=True) # [cols]\n",
    "        \n",
    "        # feature embedding layers\n",
    "        # 如果要快的話，不可以用for迴圈，要大家一起乘上parameter，參考之前冠宇的作法\n",
    "        self.num_weight = torch.nn.Parameter(torch.randn(self.num_cols, self.hidden_dim, 1, device=DEVICE), requires_grad=True) # [num_cols, hidden_dim]\n",
    "        self.num_bias = torch.nn.Parameter(torch.randn(self.num_cols, self.hidden_dim, device=DEVICE), requires_grad=True) # [hidden_dim]\n",
    "        \n",
    "        # 要怎麼知道embedding要開多大\n",
    "        self.cat_embedding = torch.nn.Embedding(sum(cat_num), self.hidden_dim, device=DEVICE, dtype=torch.float) # [sum(cat_num), hidden_dim]\n",
    "\n",
    "        # graph convolution layers\n",
    "        self.conv_1_input = torch_geometric.nn.GATConv((self.num_cols + self.cat_cols)*self.hidden_dim, self.hidden_dim)\n",
    "        self.conv_2 = torch_geometric.nn.GCNConv(self.hidden_dim, self.hidden_dim)\n",
    "\n",
    "        # prediction layer\n",
    "        self.prediction = torch.nn.Linear((self.num_cols + self.cat_cols) * self.hidden_dim , 2)\n",
    "        self.prediction_feature_importance = torch.nn.Linear((self.num_cols + self.cat_cols) * self.hidden_dim, 2)\n",
    "        \n",
    "        # normalization layer\n",
    "        self.batch_norm1 = torch.nn.BatchNorm1d(self.hidden_dim * (self.num_cols + self.cat_cols),device=DEVICE)\n",
    "        \n",
    "        self.batch_norm_FI = torch.nn.BatchNorm1d(self.number_of_columns,device=DEVICE)\n",
    "        \n",
    "    def forward(self, input_data, epoch = -1):\n",
    "        '''\n",
    "        shape of input_data: [batch_size, cols]\n",
    "        cols = num_cols + cat_cols \n",
    "        '''\n",
    "        # hyperparameters\n",
    "        COLS_NUM = self.num_cols + self.cat_cols # number of columns\n",
    "        K = round(COLS_NUM*0.3) # top-k important features\n",
    "        \n",
    "        input_data = input_data.to(DEVICE)\n",
    "        \n",
    "\n",
    "        # feature embedding\n",
    "        \n",
    "        num_data = input_data[:,:self.num_cols].unsqueeze(-1).unsqueeze(-1) \n",
    "        num_weight = self.num_weight.transpose(1,2)\n",
    "        num_bias = torch.stack([self.num_bias.clone() for _ in range(len(input_data))])\n",
    "        feature_embedding_num = torch.matmul(num_data, num_weight).transpose(2,3).squeeze(-1)\n",
    "        feature_embedding_num = feature_embedding_num + num_bias # [batch_size, hidden_dim]\n",
    "        feature_embedding_cat = self.cat_embedding(input_data[:,self.num_cols:].long()) # [batch_size, hidden_dim]\n",
    "        feature_embedding_num = torch.layer_norm(feature_embedding_num, feature_embedding_num.shape) # [batch_size, hidden_dim]\n",
    "        feature_embedding_cat = torch.layer_norm(feature_embedding_cat, feature_embedding_cat.shape) # [batch_size, hidden_dim]\n",
    "        feature_embedding = torch.cat((feature_embedding_num, feature_embedding_cat), dim=1) # [batch_size, cols, hidden_dim]\n",
    "        residual = feature_embedding.clone() # [batch_size, cols, hidden_dim]\n",
    "\n",
    "        # learn feature importance and get top-k important features\n",
    "        # print('learner', torch.where(self.feature_importance_learners_mask, self.feature_importance_learners, torch.tensor(0,device=DEVICE)).T.shape)   \n",
    "        # print('embeddign', feature_embedding.reshape(feature_embedding.shape[0],-1).shape)   \n",
    "        feature_importance_learner = torch.layer_norm(self.feature_importance_learners, self.feature_importance_learners.shape) # [cols, hidden_dim*cols]\n",
    "        # feature_importance_learners_bias = torch.layer_norm(self.feature_importance_learners_bias, self.feature_importance_learners_bias.shape) # [cols, hidden_dim*cols]\n",
    "        # feature_importance_learner = self.feature_importance_learners # [cols, hidden_dim*cols]\n",
    "        # feature_importance_learners_bias = self.feature_importance_learners_bias # [cols, hidden_dim*cols]\n",
    "        feature_importance = torch.matmul(feature_embedding.reshape(feature_embedding.shape[0],-1), torch.where(self.feature_importance_learners_mask, feature_importance_learner, torch.tensor(0,device=DEVICE)).T) # [batch_size, cols]\n",
    "        # feature_importance = feature_importance + torch.stack([feature_importance_learners_bias] * len(input_data)) # [batch_size, cols]\n",
    "        # feature_importance = self.batch_norm_FI(feature_importance)\n",
    "        # res = feature_importance.clone()    \n",
    "        # print('feature_importance',feature_importance)\n",
    "        feature_importance = torch.softmax(feature_importance, dim=1)  # [batch_size, cols]\n",
    "        # print('feature_importance',feature_importance)\n",
    "        # print('feature_importance sum',feature_importance.sum(dim=1))\n",
    "        \n",
    "        # feature_importance = torch.relu(feature_importance) # [batch_size, cols]\n",
    "\n",
    "        # print('importance', feature_importance.shape)\n",
    "        # feature_importance = self.feature_importance_learner(feature_embedding.reshape(feature_embedding.shape[0],-1)) # [batch_size, cols]\n",
    "\n",
    "        values, row_indices = torch.topk(feature_importance, k = K) # (value: [batch_size, k], indices: [batch_size, k])\n",
    "        \n",
    "        # print(values[0])\n",
    "        # print(values[:,[-1]])\n",
    "        # print(row_indices[0])\n",
    "        importance_topk = torch.where(feature_importance >= values[:,[-1]], feature_importance, torch.zeros(feature_importance.shape,device=DEVICE)) # [batch_size, cols]\n",
    "        # print(importance_topk[0])\n",
    "\n",
    "        # 新問題：怎麼讓生成每張圖的速度變快，做到評型化\n",
    "        # 解法，先不考慮評型化，每張圖用for迴圈做，\n",
    "        # 新問題，現在每個colum只用一個數字表示，不夠大，應該要加一個feature embedding layer\n",
    "        \n",
    "        # copy importance_topk \n",
    "        importance_topk = torch.stack([importance_topk.clone() for _ in range(COLS_NUM)], dim=0) # [cols, batch_size, cols]\n",
    "        # importance_topk = importance_topk.unsqueeze(2).repeat(1, 1, COLS_NUM).reshape(COLS_NUM,-1, COLS_NUM)\n",
    "        \n",
    "        # for each column, drop columns and row thar are not important in target column\n",
    "        # which means, importance_topk[target_col].T[target_col]==0 should be dropped\n",
    "        processed_data = [] \n",
    "        loss_graphs = 0\n",
    "        for target_col in range(COLS_NUM):\n",
    "            \n",
    "            importance_topk_current = importance_topk[target_col] # [batch_size, cols]\n",
    "            indices = importance_topk_current.T[target_col].nonzero().T[0] # selected samples' indices\n",
    "            importance_topk_current = importance_topk_current[importance_topk_current.T[target_col]>0]# [????, cols]\n",
    "        \n",
    "        \n",
    "            # for target column, set its importance to 0. so that it will not be fully connected graph\n",
    "            # copy target column\n",
    "            tmp = torch.clone(importance_topk_current[:,target_col]) # [????], save for future weighted sum\n",
    "            importance_topk_current[:,target_col] = 0 # [????, cols]\n",
    "            \n",
    "            # multiply to get weighted adj\n",
    "            weighted_adj = torch.matmul(importance_topk_current, importance_topk_current.T) # [batch_size, cols] * [cols, batch_size] = [batch_size, batch_size]\n",
    "            # prune the diagonal\n",
    "            weighted_adj = weighted_adj - torch.diag(weighted_adj.diagonal())\n",
    "            # construct graph edge\n",
    "            edge_index = weighted_adj.nonzero().T  # [2, num_edges]\n",
    "            edge_wight = weighted_adj[edge_index[0], edge_index[1]] # [num_edges]\n",
    "            \n",
    "            # construct node feature\n",
    "            # features = input_data[indices] \n",
    "            importance_topk_current[:,target_col] = tmp # [????, cols]\n",
    "            try:\n",
    "                features = (feature_embedding[indices] * importance_topk_current.unsqueeze(2)).reshape(len(indices),-1) # [????, cols*hidden_dim]\n",
    "                # features = (feature_embedding[indices] ).reshape(len(indices),-1) # [????, cols*hidden_dim]\n",
    "            except:\n",
    "                # if there is no nodes in the graph, continue\n",
    "                # print('graph',target_col, 'has no nodes')\n",
    "                target_col_data = torch.zeros(feature_embedding.size(0), self.hidden_dim, device=DEVICE) # [batch_size, hidden_dim]\n",
    "                processed_data.append(target_col_data)\n",
    "                continue\n",
    "\n",
    "            \n",
    "            # construct graph \n",
    "            data = Data(x=features, edge_index=edge_index, edge_weight=edge_wight, indices=indices) \n",
    "            \n",
    "            # if epoch % 20 == 0 and epoch != -1:\n",
    "            #     try:\n",
    "            #         print('in feature graph',target_col, 'node num', len(indices), 'edge num', len(edge_index[0]), 'ratio', round(len(edge_index[0])/(len(indices)**2),3))\n",
    "            #     except:\n",
    "            #         print('in feature graph',target_col, 'node num', len(indices), 'edge num', len(edge_index[0]), 'ratio', 'NA')\n",
    "                \n",
    "                \n",
    "            # apply GCN\n",
    "            # x = self.batch_norm1(data.x)\n",
    "            x = torch.layer_norm(data.x, data.x.shape) # [num_nodes, hidden_dim]\n",
    "            x = self.conv_1_input(x, data.edge_index, data.edge_weight)  # [num_nodes, hidden_dim]\n",
    "            x = torch.relu(x)\n",
    "\n",
    "            target_col_data = torch.zeros(feature_embedding.size(0), self.hidden_dim, device=DEVICE) # [batch_size, hidden_dim]\n",
    "            target_col_data.index_add_(0, indices , x) # [batch_size, hidden_dim]\n",
    "            processed_data.append(target_col_data)\n",
    "            \n",
    "            # get readout of the graph\n",
    "            # readout = x[:,start:end] # [???, hidden_dim]\n",
    "            readout = x # [???, hidden_dim]\n",
    "            readout = torch.sum(readout, dim=0) # [hidden_dim]\n",
    "            feature_importance_weight = torch.masked_select(self.feature_importance_learners, self.feature_importance_learners_mask).reshape(self.number_of_columns,-1)[target_col] # [hidden_dim]\n",
    "            \n",
    "            # cacuate cosine similarity between readout and feature importance\n",
    "            readout = torch.layer_norm(readout, readout.shape) # [hidden_dim]\n",
    "            feature_importance_weight = torch.layer_norm(feature_importance_weight, feature_importance_weight.shape)\n",
    "            # print(readout.shape)\n",
    "            # print(feature_importance_weight.shape)  \n",
    "            loss_graph = -1 * torch.cosine_similarity(readout, feature_importance_weight, dim=0) + 1  # [1], range from 0 to 2\n",
    "            loss_graphs += loss_graph\n",
    "            # print('loss_graph',target_col,loss_graph)\n",
    "\n",
    "        if epoch % 200 == 0 :\n",
    "            print('feature_importance',sum(feature_importance)/len(input_data))\n",
    "        \n",
    "        processed_data = torch.stack(processed_data, dim=0) # [cols, batch_size, hidden_dim]\n",
    "        processed_data = torch.permute(processed_data, (1,0,2)) # [batch_size, cols, hidden_dim]\n",
    "        # processed_data = torch.cat((processed_data, residual), dim=1) # [batch_size, cols*2, hidden_dim]\n",
    "        processed_data = processed_data.reshape(processed_data.shape[0],-1) # [batch_size, cols*hidden_dim]\n",
    "            \n",
    "\n",
    "        # make prediction\n",
    "        output = self.prediction(processed_data) # [num_nodes, label_cols]\n",
    "        \n",
    "        # weighted feature \n",
    "        feature_embedding_ = feature_embedding.clone().detach()\n",
    "        features_ = (feature_embedding_ * feature_importance.unsqueeze(2)).reshape(len(input_data),-1)# [????, cols*hidden_dim]\n",
    "        features_ = torch.layer_norm(features_, features_.shape) # [????, cols*hidden_dim]\n",
    "        output_feature_importance = self.prediction_feature_importance(features_) # [num_nodes, label_cols]\n",
    "        \n",
    "        \n",
    "        # # caculate loss from CancelOut\n",
    "        # a = torch.masked_select(self.feature_importance_learners, self.feature_importance_learners_mask).reshape(self.number_of_columns,-1) \n",
    "        # print(a.shape)\n",
    "        # a = torch.sum(a, dim=1) / self.number_of_columns # [cols]\n",
    "        # loss1 = -0.02 * torch.var(a)\n",
    "        # # print(loss1)\n",
    "        # loss2 = 0.001 * torch.sqrt(torch.sum(a*a))\n",
    "        # # print(loss2)\n",
    "    \n",
    "        \n",
    "        return output, loss_graphs/self.number_of_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model = K_graph(NUM, CAT, [LABEL], cat_num).to(DEVICE)\n",
    "optimizer = torch.optim.SGD(the_model.parameters(), lr=0.01)\n",
    "\n",
    "# output = the_model(x[:1000], epoch=200)\n",
    "# loss = torch.nn.functional.cross_entropy(output, y[:1000])\n",
    "# loss.backward()\n",
    "# # print(the_model.num_weight.grad)\n",
    "\n",
    "# # print(the_model.feature_importance_learners_bias.grad)\n",
    "# # print(the_model.feature_importance_learners.grad.shape)\n",
    "# # print(the_model.feature_importance_learners.grad)\n",
    "# print(sum(the_model.feature_importance_learners.grad))\n",
    "\n",
    "# # print(the_model.feature_importance_learners.grad[0].shape)\n",
    "# # # print(the_model.feature_importance_learners_mask.grad)\n",
    "\n",
    "# optimizer.step()\n",
    "data_count = 10\n",
    "# random pick data\n",
    "indices = torch.randperm(len(x))[:data_count]\n",
    "print(indices)\n",
    "train_data = x[indices]\n",
    "train_label = y[indices]\n",
    "for i in range(1):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output, loss_graphs = the_model(train_data[:data_count], epoch=200)\n",
    "    loss = torch.nn.functional.cross_entropy(output, train_label[:data_count])*14\n",
    "    # print((loss + loss_graphs), loss, loss_graphs)\n",
    "    loss = loss + loss_graphs\n",
    "        # loss = loss + loss1 + loss2\n",
    "    loss.backward()\n",
    "    # torch.nn.utils.clip_grad_norm_(the_model.parameters(), 1)\n",
    "    print(((the_model.feature_importance_learners.grad).abs().max(dim=1)[0]))\n",
    "    # print(((the_model.prediction.weight.grad).max(dim=1)[0]))\n",
    "    # print(((the_model.prediction_feature_importance.weight.grad).abs().max(dim=1)[0]))\n",
    "    # print(len(the_model.feature_importance_learners.grad))\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('-----------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchviz\n",
    "plot = torchviz.make_dot(loss, params=dict(the_model.named_parameters()))\n",
    "plot.render(\"attached\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def train(model, optimizer, x, y):\n",
    "    epoch = 5000\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out, out_graph = model(x, i+1)\n",
    "        loss = torch.nn.functional.cross_entropy(out, y) * model.number_of_columns\n",
    "        loss = loss + out_graph\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # calculate accuracy\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = pred.eq(y).sum().item()\n",
    "        acc = correct / len(y)\n",
    "        # caculate AUC\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        auc = roc_auc_score(y.cpu().numpy(), pred.cpu().numpy())\n",
    "        print('epoch',i,'loss',loss.item(),'acc',acc,'AUC1',auc, 'pred', sum(pred),)\n",
    "    return \n",
    "the_model = K_graph(NUM, CAT, [LABEL], cat_num).to(DEVICE)\n",
    "train(the_model, torch.optim.SGD(the_model.parameters(), lr=0.001), x[:1000], y[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# playground\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## matrix mul to get ADJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[0,1,1],\n",
    "                  [0,1,1],\n",
    "                  [0,1,1],\n",
    "                  [1,1,0]], dtype=torch.float)\n",
    "print(A.shape,A.T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADJ = torch.matmul(A, A.T)\n",
    "print(ADJ)\n",
    "# convert to edge_index\n",
    "edge_index = torch.tensor([[i, j] for i in range(ADJ.shape[0]) for j in range(ADJ.shape[1]) if (ADJ[i,j] >= 1) and (i!=j)], dtype=torch.long).t()\n",
    "print(edge_index)\n",
    "# make graph with weighted adjacency matrix\n",
    "data = Data(edge_index=edge_index)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(data):\n",
    "    import networkx as nx\n",
    "    import matplotlib.pyplot as plt\n",
    "    from torch_geometric.utils import to_networkx\n",
    "    import torch_geometric.datasets as datasets\n",
    "\n",
    "    G = to_networkx(data)\n",
    "\n",
    "    # 绘制 NetworkX 图\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    pos = nx.spring_layout(G)  # 定义节点位置\n",
    "    nx.draw(G, pos, with_labels=True, node_size=200, node_color='skyblue', font_weight='bold', font_size=8, edge_color='gray')\n",
    "    plt.title('Graph Visualization')\n",
    "    plt.show()\n",
    "visualize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup of old code\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, NUM, CAT, LABEL):\n",
    "        super(Net, self).__init__()\n",
    "        '''\n",
    "        num_cols: number of numerical columns\n",
    "        cat_cols: number of categorical columns\n",
    "        label_cols: number of label columns\n",
    "        '''\n",
    "        self.hidden_dim = 128\n",
    "        \n",
    "        # order: num -> cat -> label\n",
    "        self.num_cols = len(NUM)\n",
    "        self.cat_cols = len(CAT)\n",
    "        self.label_cols = len(LABEL)\n",
    "        \n",
    "        # layers\n",
    "        self.feature_importance_learner = torch.nn.Linear(self.num_cols + self.cat_cols, self.num_cols + self.cat_cols)\n",
    "        self.conv_1_input = torch_geometric.nn.GCNConv(self.num_cols + self.cat_cols, self.hidden_dim)\n",
    "        self.conv_2 = torch_geometric.nn.GCNConv(self.hidden_dim, self.hidden_dim)\n",
    "\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        '''\n",
    "        shape of input_data: [batch_size, cols]\n",
    "        cols = num_cols + cat_cols \n",
    "        '''\n",
    "        # print(input_data)\n",
    "        \n",
    "        # learn feature importance and get top-k important features\n",
    "        feature_importance = self.feature_importance_learner(input_data) # [batch_size, cols]\n",
    "        values, row_indices = torch.topk(feature_importance, k=3) # (value: [batch_size, k], indices: [batch_size, k])\n",
    "        # row_indices = feature_importance_result[1] # [batch_size, k]\n",
    "        # print(feature_importance.shape)\n",
    "        # print(row_indices)\n",
    "        \n",
    "        \n",
    "        importance_topk = torch.where(feature_importance >= values[:,[-1]], feature_importance, torch.zeros(feature_importance.shape,device=DEVICE)) # [batch_size, cols]\n",
    "            \n",
    "        # # construct tensor of top-k importance table, where importance_topk[i][j] = 1 if j is in top-k important features of i\n",
    "        # importance_topk = torch.zeros(input_data.size(0), self.num_cols + self.cat_cols, device=DEVICE) # [batch_size, cols]\n",
    "        # importance_topk.scatter_(1, row_indices, 1) # [batch_size, cols]\n",
    "        # # print(importance_topk)\n",
    "        # # print(importance_topk.shape)\n",
    "        \n",
    "        # multiply to get weighted adj\n",
    "        weighted_adj = torch.matmul(importance_topk, importance_topk.T) # [batch_size, cols] * [cols, batch_size] = [batch_size, batch_size]\n",
    "        # print(weighted_adj)\n",
    "        # print(weighted_adj.shape)\n",
    "        \n",
    "        # construct graph\n",
    "        edge_index = weighted_adj.nonzero().T  # [2, num_edges]\n",
    "        edge_wight = weighted_adj[edge_index[0], edge_index[1]] # [num_edges]\n",
    "        data = Data(x=input_data, edge_index=edge_index, edge_weight=edge_wight) \n",
    "        # print(edge_index, edge_index.shape)\n",
    "        # print(edge_wight, edge_wight.shape)\n",
    "        \n",
    "        # apply GCN\n",
    "        # print(data.x.shape)  # [num_nodes, num_cols]\n",
    "        x = self.conv_1_input(data.x, data.edge_index, data.edge_weight)  # [num_nodes, hidden_dim]\n",
    "        # print(x.shape)\n",
    "        x = self.conv_2(x, data.edge_index, data.edge_weight) # [num_nodes, hidden_dim]\n",
    "        # print(x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model = Net(NUM, CAT, LABEL).to(DEVICE)\n",
    "the_model(x[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, NUM, CAT, LABEL):\n",
    "        super(Net, self).__init__()\n",
    "        '''\n",
    "        num_cols: number of numerical columns\n",
    "        cat_cols: number of categorical columns\n",
    "        label_cols: number of label columns\n",
    "        '''\n",
    "        self.hidden_dim = 128\n",
    "        \n",
    "        # order: num -> cat -> label\n",
    "        self.num_cols = len(NUM)\n",
    "        self.cat_cols = len(CAT)\n",
    "        self.label_cols = len(LABEL)\n",
    "        \n",
    "        # layers\n",
    "        self.feature_importance_learner = torch.nn.Linear(self.num_cols + self.cat_cols, self.num_cols + self.cat_cols)\n",
    "        self.conv_1_input = torch_geometric.nn.GCNConv(self.num_cols + self.cat_cols, self.hidden_dim)\n",
    "        self.conv_2 = torch_geometric.nn.GCNConv(self.hidden_dim, self.hidden_dim)\n",
    "\n",
    "        # prediction layer\n",
    "        self.prediction = torch.nn.Linear(self.hidden_dim, self.label_cols)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        '''\n",
    "        shape of input_data: [batch_size, cols]\n",
    "        cols = num_cols + cat_cols \n",
    "        '''\n",
    "        print('input_data',input_data)\n",
    "        \n",
    "        # learn feature importance and get top-k important features\n",
    "        feature_importance = self.feature_importance_learner(input_data) # [batch_size, cols]\n",
    "        feature_importance_result = torch.topk(feature_importance, k=3) # (value: [batch_size, k], indices: [batch_size, k])\n",
    "        row_indices = feature_importance_result[1] # [batch_size, k]\n",
    "        print('feature_importance shape',feature_importance.shape)\n",
    "        print('row_indices',row_indices)\n",
    "        \n",
    "        # construct tensor of top-k importance table, where importance_topk[i][j] = 1 if j is in top-k important features of i\n",
    "        importance_topk = torch.zeros(input_data.size(0), self.num_cols + self.cat_cols, device=DEVICE) # [batch_size, cols]\n",
    "        importance_topk.scatter_(1, row_indices, 1) # [batch_size, cols]\n",
    "        print('importance_topk',importance_topk)\n",
    "        print('importance_topk.shape',importance_topk.shape)\n",
    "        # 現在的問題：沒有針對各個feature level graph做node的裁切，導致生成一張超大的圖\n",
    "        # 上述問題已解決\n",
    "        # 新問題：怎麼讓生成每張圖的速度變快，做到評型化\n",
    "        \n",
    "        \n",
    "        # drop columns and row thar are not important in target column\n",
    "        # which means, importance_topk.T[target_col]==0 should be dropped\n",
    "        target_col = 0\n",
    "        indices = importance_topk.T[target_col].nonzero().T # selected samples' indices\n",
    "        importance_topk = importance_topk[importance_topk.T[target_col]==1]# [????, cols]\n",
    "        print('importance_topk.shape',importance_topk.shape)  \n",
    "        print('importance_topk',importance_topk)  \n",
    "        print('indices', len(indices[0]), indices)\n",
    "        \n",
    "        \n",
    "        # for target column, set its importance to 0. so that it will not be all connected\n",
    "        importance_topk[:,target_col] = 0 # [batch_size, cols]\n",
    "        \n",
    "        # multiply to get weighted adj\n",
    "        weighted_adj = torch.matmul(importance_topk, importance_topk.T) # [batch_size, cols] * [cols, batch_size] = [batch_size, batch_size]\n",
    "        # prune the diagonal\n",
    "        weighted_adj = weighted_adj - torch.diag(weighted_adj.diagonal())\n",
    "        print('weighted_adj',weighted_adj)\n",
    "        print('weighted_adj.shape',weighted_adj.shape)\n",
    "        \n",
    "        # construct graph\n",
    "        edge_index = weighted_adj.nonzero().T  # [2, num_edges]\n",
    "        edge_wight = weighted_adj[edge_index[0], edge_index[1]] # [num_edges]\n",
    "        data = Data(x=input_data, edge_index=edge_index, edge_weight=edge_wight) \n",
    "        print('edge_index', edge_index, edge_index.shape)\n",
    "        print('edge_wight', edge_wight, edge_wight.shape)\n",
    "        \n",
    "        # apply GCN\n",
    "        print(data.x.shape)  # [num_nodes, num_cols]\n",
    "        x = self.conv_1_input(data.x, data.edge_index, data.edge_weight)  # [num_nodes, hidden_dim]\n",
    "        print(x.shape)\n",
    "        x = self.conv_2(x, data.edge_index, data.edge_weight) # [num_nodes, hidden_dim]\n",
    "        print(x.shape)\n",
    "\n",
    "        # make prediction\n",
    "        x = self.prediction(x) # [num_nodes, label_cols]\n",
    "        print(x.shape) \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = torch.tensor([\n",
    "        [0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.],\n",
    "        [0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.],\n",
    "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0.],\n",
    "        [1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.],\n",
    "        [0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.]])\n",
    "print(B.shape)\n",
    "print(torch.stack([B]*3, dim=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[\n",
    "        [0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.],\n",
    "        [0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.],\n",
    "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0.],\n",
    "        [1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.],\n",
    "        [0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.]],\n",
    "        [[0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.],\n",
    "        [0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.],\n",
    "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0.],\n",
    "        [1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.],\n",
    "        [0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.]]])\n",
    "print(A.shape)\n",
    "print(A[0][A[0].T[0]==1])\n",
    "# get indince\n",
    "print(A[0].T[0].nonzero().T)\n",
    "\n",
    "\n",
    "print(A[1][A[1].T[1]==1])\n",
    "# get indince\n",
    "print(A[1].T[1].nonzero().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vaild_samples(A, a):\n",
    "    target_col = A.index(a)\n",
    "    return a[a.T[target_col]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, a in enumerate(A):\n",
    "    print(get_vaild_samples(A,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.apply_(lambda x: get_vaild_samples(A,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0][A[0].T[0]==1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(5, 3)\n",
    "print(x)\n",
    "t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n",
    "print(t)\n",
    "index = torch.tensor([0, 4, 2])\n",
    "print(index)\n",
    "print(x.index_add_(0, index, t))\n",
    "print(x.index_add_(0, index, t, alpha=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 给定的张量\n",
    "input_tensor = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# 创建一个全零的目标张量，形状为(3, 6)，6 是输入张量的列数\n",
    "target_tensor = torch.zeros(3, 6, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# 生成对应的行索引和列索引\n",
    "col_indices = input_tensor.clone()-1\n",
    "print(col_indices)\n",
    "\n",
    "# 创建一个新的变量来保存结果，避免在原始张量上进行 in-place 操作\n",
    "result = target_tensor.clone()\n",
    "\n",
    "# 使用 scatter_add_ 将 input_tensor 的值相加到 result 中\n",
    "print(result.shape)\n",
    "print(col_indices.shape)\n",
    "print(input_tensor.shape)\n",
    "result.scatter_add_(1, col_indices, input_tensor.float())\n",
    "\n",
    "# 将 result 赋值给 target_tensor，从而保留梯度\n",
    "target_tensor = result.detach().requires_grad_()\n",
    "\n",
    "print(target_tensor)\n",
    "print(target_tensor.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 给定的张量\n",
    "input_tensor = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "# index = torch.tensor([[0, 1], [2, 3], [4, 5]])\n",
    "index = torch.arange(6).view(3, 2)\n",
    "\n",
    "# 创建一个全零的目标张量，形状为(3, 6)，6 是输入张量的列数\n",
    "target_tensor = torch.zeros(3, 6, dtype=torch.int64)\n",
    "\n",
    "# 使用 scatter_ 将 input_tensor 的值根据索引分散到 target_tensor 中\n",
    "target_tensor.scatter_(1, index, input_tensor)\n",
    "\n",
    "print(target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 给定的张量\n",
    "input_tensor = torch.zeros((3, 2))\n",
    "# index = torch.tensor([[0, 1], [2, 3], [4, 5]])\n",
    "index = torch.arange(6).view(3, 2)\n",
    "\n",
    "# 创建一个全零的目标张量，形状为(3, 6)，6 是输入张量的列数\n",
    "target_tensor = torch.ones(3, 6).scatter_(1, index, torch.zeros((3, 2))).bool()\n",
    "\n",
    "\n",
    "print(target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.arange(4*3*2).reshape(4,3,2)\n",
    "print(A)\n",
    "A = A.reshape(4,-1)\n",
    "print(A)\n",
    "mask = torch.zeros(3,3*2).scatter_(1, torch.tensor([[0, 1],[ 2, 3],[ 4, 5]]), torch.ones((4, 6))).bool()\n",
    "print(mask)\n",
    "learner = torch.zeros(3,3*2)\n",
    "print(learner)\n",
    "learner=learner.masked_fill_(mask, 1).T\n",
    "print(learner)\n",
    "print(A.shape, learner.shape)\n",
    "A = torch.matmul(A.float(), learner.float())\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# 定义一个简单的模型\n",
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# 初始化模型和优化器\n",
    "model = SimpleModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 定义输入\n",
    "input_data = Variable(torch.randn(5, 2), requires_grad=True)\n",
    "\n",
    "# 前向传播\n",
    "output = model(input_data)\n",
    "\n",
    "# 反向传播\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "\n",
    "# 定义一个用于查看梯度的函数\n",
    "def print_grad(grad):\n",
    "    print('grad is',grad)\n",
    "\n",
    "# 注册hook来查看梯度\n",
    "model.linear.weight.register_hook(print_grad)\n",
    "model.linear.bias.register_hook(print_grad)\n",
    "\n",
    "# 进行优化器的一步更新\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.linear.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.tensor([0., 0., 0.], requires_grad=True)\n",
    "h = v.register_hook(lambda grad: grad * 2)  # double the gradient\n",
    "v.backward(torch.tensor([1., 2., 3.]))\n",
    "v.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature importance tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = Variable(torch.randn(10, 14), requires_grad=True)\n",
    "learner = torch.nn.Linear(14, 14)\n",
    "feature_importance = learner(input_data) # [batch_size, cols]\n",
    "feature_importance_result = torch.topk(feature_importance, k = 3) # (value: [batch_size, k], indices: [batch_size, k])\n",
    "row_indices = feature_importance_result[1] # [batch_size, k]\n",
    "print('feature_importance shape',feature_importance.shape)\n",
    "print('row_indices',row_indices)\n",
    "\n",
    "# construct tensor of top-k importance table, where importance_topk[i][j] = 1 if j is in top-k important features of i\n",
    "importance_topk = torch.zeros(input_data.size(0), 14) # [batch_size, cols]\n",
    "importance_topk.scatter_(1, row_indices, 1) # [batch_size, cols]\n",
    "print('importance_topk',importance_topk)\n",
    "print('importance_topk.shape',importance_topk.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = Variable(torch.randn(10, 14), requires_grad=True)\n",
    "learner = torch.nn.Linear(14, 14)\n",
    "feature_importance = learner(input_data) # [batch_size, cols]\n",
    "values, indices = torch.topk(feature_importance, k = 3) # (value: [batch_size, k], indices: [batch_size, k])\n",
    "print(values, indices)\n",
    "feature_importance = torch.where(feature_importance >= values[:,[-1]], feature_importance, torch.zeros(feature_importance.shape))\n",
    "print(feature_importance)\n",
    " # multiply to get weighted adj\n",
    "weighted_adj = torch.matmul(feature_importance, feature_importance.T) # [batch_size, cols] * [cols, batch_size] = [batch_size, batch_size]\n",
    "print(weighted_adj)\n",
    "print(weighted_adj.shape)\n",
    "\n",
    "# # construct graph\n",
    "edge_index = weighted_adj.nonzero().T  # [2, num_edges]\n",
    "edge_wight = weighted_adj[edge_index[0], edge_index[1]] # [num_edges]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有prediction 的cell\n",
    "class K_graph(torch.nn.Module):\n",
    "    def __init__(self, NUM, CAT, LABEL, cat_num):\n",
    "        super(K_graph, self).__init__()\n",
    "        '''\n",
    "        num_cols: number of numerical columns\n",
    "        cat_cols: number of categorical columns\n",
    "        label_cols: number of label columns\n",
    "        cat_num: number of unique value of each categorical columns\n",
    "        '''\n",
    "        self.hidden_dim = 128\n",
    "        \n",
    "        # order: num -> cat -> label\n",
    "        self.num_cols = len(NUM)\n",
    "        self.cat_cols = len(CAT)\n",
    "        self.label_cols = len(LABEL)\n",
    "        \n",
    "        # layers\n",
    "        self.feature_importance_learner = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.hidden_dim * (self.num_cols + self.cat_cols), self.num_cols + self.cat_cols),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        # feature embedding layers\n",
    "        # 如果要快的話，不可以用for迴圈，要大家一起乘上parameter，參考之前冠宇的作法\n",
    "        self.num_weight = torch.nn.Parameter(torch.randn(self.num_cols, self.hidden_dim, 1, device=DEVICE), requires_grad=True) # [num_cols, hidden_dim]\n",
    "        self.num_bias = torch.nn.Parameter(torch.randn(self.num_cols, self.hidden_dim, device=DEVICE), requires_grad=True) # [hidden_dim]\n",
    "        \n",
    "        # 要怎麼知道embedding要開多大\n",
    "        self.cat_embedding = torch.nn.Embedding(sum(cat_num), self.hidden_dim, device=DEVICE, dtype=torch.float) # [sum(cat_num), hidden_dim]\n",
    "\n",
    "        # graph convolution layers\n",
    "        self.conv_1_input = torch_geometric.nn.GATConv((self.num_cols + self.cat_cols)*self.hidden_dim, self.hidden_dim)\n",
    "        self.conv_2 = torch_geometric.nn.GCNConv(self.hidden_dim, self.hidden_dim)\n",
    "\n",
    "        # prediction layer\n",
    "        self.prediction = torch.nn.Linear((self.num_cols + self.cat_cols) * self.hidden_dim, 2)\n",
    "        \n",
    "        # normalization layer\n",
    "        self.batch_norm1 = torch.nn.BatchNorm1d(self.hidden_dim * (self.num_cols + self.cat_cols),device=DEVICE)\n",
    "        \n",
    "    def forward(self, input_data, epoch = -1):\n",
    "        '''\n",
    "        shape of input_data: [batch_size, cols]\n",
    "        cols = num_cols + cat_cols \n",
    "        '''\n",
    "        # hyperparameters\n",
    "        COLS_NUM = self.num_cols + self.cat_cols # number of columns\n",
    "        K = round(COLS_NUM*0.3) # top-k important features\n",
    "        \n",
    "        input_data = input_data.to(DEVICE)\n",
    "        \n",
    "\n",
    "        # feature embedding\n",
    "        \n",
    "        num_data = input_data[:,:self.num_cols].unsqueeze(-1).unsqueeze(-1) \n",
    "        num_weight = self.num_weight.transpose(1,2)\n",
    "        num_bias = torch.stack([self.num_bias] * len(input_data))\n",
    "        feature_embedding_num = torch.matmul(num_data, num_weight).transpose(2,3).squeeze(-1)\n",
    "        feature_embedding_num = feature_embedding_num + num_bias # [batch_size, hidden_dim]\n",
    "        feature_embedding_cat = self.cat_embedding(input_data[:,self.num_cols:].long()) # [batch_size, hidden_dim]\n",
    "        feature_embedding = torch.cat((feature_embedding_num, feature_embedding_cat), dim=1) # [batch_size, cols, hidden_dim]\n",
    "\n",
    "        # learn feature importance and get top-k important features\n",
    "        feature_importance = self.feature_importance_learner(feature_embedding.reshape(feature_embedding.shape[0],-1)) # [batch_size, cols]\n",
    "\n",
    "        values, row_indices = torch.topk(feature_importance, k = K) # (value: [batch_size, k], indices: [batch_size, k])\n",
    "        \n",
    "        \n",
    "        importance_topk = torch.where(feature_importance >= values[:,[-1]], feature_importance, torch.zeros(feature_importance.shape,device=DEVICE)) # [batch_size, cols]\n",
    "        \n",
    "        # 現在的問題：沒有針對各個feature level graph做node的裁切，導致生成一張超大的圖\n",
    "        # 上述問題已解決\n",
    "        # 新問題：怎麼讓生成每張圖的速度變快，做到評型化\n",
    "        # 解法，先不考慮評型化，每張圖用for迴圈做，\n",
    "        # 新問題，現在每個colum只用一個數字表示，不夠大，應該要加一個feature embedding layer\n",
    "        \n",
    "        # copy importance_topk \n",
    "        importance_topk = torch.stack([importance_topk] * COLS_NUM, dim=0) # [cols, batch_size, cols]\n",
    "        \n",
    "        \n",
    "        # for each column, drop columns and row thar are not important in target column\n",
    "        # which means, importance_topk[target_col].T[target_col]==0 should be dropped\n",
    "        processed_data = [] \n",
    "        for target_col in range(COLS_NUM):\n",
    "            \n",
    "            importance_topk_current = importance_topk[target_col] # [batch_size, cols]\n",
    "            indices = importance_topk_current.T[target_col].nonzero().T[0] # selected samples' indices\n",
    "            importance_topk_current = importance_topk_current[importance_topk_current.T[target_col]>0]# [????, cols]\n",
    "        \n",
    "        \n",
    "            # for target column, set its importance to 0. so that it will not be fully connected graph\n",
    "            # copy target column\n",
    "            tmp = torch.clone(importance_topk_current[:,target_col]) # [????], save for future weighted sum\n",
    "            importance_topk_current[:,target_col] = 0 # [batch_size, cols]\n",
    "            \n",
    "            # multiply to get weighted adj\n",
    "            weighted_adj = torch.matmul(importance_topk_current, importance_topk_current.T) # [batch_size, cols] * [cols, batch_size] = [batch_size, batch_size]\n",
    "            # prune the diagonal\n",
    "            weighted_adj = weighted_adj - torch.diag(weighted_adj.diagonal())\n",
    "            # construct graph edge\n",
    "            edge_index = weighted_adj.nonzero().T  # [2, num_edges]\n",
    "            edge_wight = weighted_adj[edge_index[0], edge_index[1]] # [num_edges]\n",
    "            \n",
    "            # construct node feature\n",
    "            # features = input_data[indices] \n",
    "            importance_topk_current[:,target_col] = tmp # [????, cols]\n",
    "            try:\n",
    "                # features = (feature_embedding[indices] * importance_topk_current.unsqueeze(2)).reshape(len(indices),-1) # [????, cols*hidden_dim]\n",
    "                features = (feature_embedding[indices] ).reshape(len(indices),-1) # [????, cols*hidden_dim]\n",
    "            except:\n",
    "                # if there is no nodes in the graph, continue\n",
    "                target_col_data = torch.zeros(feature_embedding.size(0), self.hidden_dim, device=DEVICE) # [batch_size, hidden_dim]\n",
    "                processed_data.append(target_col_data)\n",
    "                continue\n",
    "\n",
    "            \n",
    "            # construct graph \n",
    "            data = Data(x=features, edge_index=edge_index, edge_weight=edge_wight, indices=indices) \n",
    "            \n",
    "            if epoch % 1000 == 0 and epoch != -1:\n",
    "                try:\n",
    "                    print('in feature graph',target_col, 'node num', len(indices), 'edge num', len(edge_index[0]), 'ratio', round(len(edge_index[0])/(len(indices)**2),3))\n",
    "                except:\n",
    "                    print('in feature graph',target_col, 'node num', len(indices), 'edge num', len(edge_index[0]), 'ratio', 'NA')\n",
    "                \n",
    "                \n",
    "            # apply GCN\n",
    "            # x = self.batch_norm1(data.x)\n",
    "            x = torch.layer_norm(data.x, data.x.shape) # [num_nodes, hidden_dim]\n",
    "            x = self.conv_1_input(x, data.edge_index, data.edge_weight)  # [num_nodes, hidden_dim]\n",
    "            x = torch.relu(x)\n",
    "            \n",
    "\n",
    "            target_col_data = torch.zeros(feature_embedding.size(0), self.hidden_dim, device=DEVICE) # [batch_size, hidden_dim]\n",
    "            target_col_data.index_add_(0, indices , x) # [batch_size, hidden_dim]\n",
    "            processed_data.append(target_col_data)\n",
    "\n",
    "        \n",
    "        processed_data = torch.stack(processed_data, dim=0) # [cols, batch_size, hidden_dim]\n",
    "        processed_data = torch.permute(processed_data, (1,0,2)) # [batch_size, cols, hidden_dim]\n",
    "        processed_data = processed_data.reshape(processed_data.shape[0],-1) # [batch_size, cols*hidden_dim]\n",
    "            \n",
    "\n",
    "        # make prediction\n",
    "        output = self.prediction(processed_data) # [num_nodes, label_cols]\n",
    "        \n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
